{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ANC Vertex: Wikipedia + FAISS + Vertex AI + StateGraph\n",
        "\n",
        "This notebook combines two Vertex AI workflows in one place:\n",
        "\n",
        "- Baseline notebook pipeline: Wikipedia -> chunking -> FAISS -> Vertex-hosted Q&A with `ChatVertexAI`\n",
        "- Canonical orchestration diagram + run flow via shared `naturalist_companion.stategraph_shared`\n",
        "\n",
        "Provider mapping used here:\n",
        "- `ChatOllama` -> `ChatVertexAI`\n",
        "- `OllamaEmbeddings` -> `VertexAIEmbeddings`\n",
        "- Same `WikipediaLoader` + `FAISS` retrieval flow\n"
      ],
      "id": "8e27d759e35adcd0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites (Run First)\n",
        "\n",
        "### Google Cloud / Vertex AI setup\n",
        "\n",
        "- Authenticate locally before running notebook cells: `gcloud auth application-default login`\n",
        "- Set project and region env vars as needed: `GOOGLE_CLOUD_PROJECT` and `VERTEX_LOCATION`\n",
        "- Ensure Vertex AI API is enabled for your project.\n",
        "- Optional: set `GCP_PROJECT` if you use that env var name instead of `GOOGLE_CLOUD_PROJECT`.\n",
        "\n",
        "### Python dependency notes\n",
        "\n",
        "- Run the next code cell in a fresh kernel to install notebook dependencies.\n",
        "- If you hit `TqdmWarning: IProgress not found`, run `%pip install -q ipywidgets jupyterlab_widgets` and restart the kernel.\n",
        "- If you hit macOS OpenMP kernel crashes, launch Jupyter with `KMP_DUPLICATE_LIB_OK=TRUE`.\n",
        "\n",
        "### IDE notebook stability tips\n",
        "\n",
        "- Ensure the notebook kernel points to this project interpreter: `/Users/ryan/Developer/naturalist-companion/.venv/bin/python`\n",
        "- After dependency changes, use kernel restart then run all cells from top.\n",
        "- Keep one notebook kernel active at a time while testing provider configs.\n"
      ],
      "id": "9be91c2e49c297c0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-06T17:14:14.101429Z",
          "start_time": "2026-02-06T17:14:14.094633Z"
        }
      },
      "source": [
        "# Uncomment in fresh environments:\n",
        "\n",
        "# %pip install -q -r ../requirements-gcp-dev.txt\n",
        "\n",
        "\n",
        "# %pip install -q ipywidgets\n",
        "\n",
        "# %pip install -q jupyterlab_widgets\n"
      ],
      "id": "c89ef19ee6f0156f",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Databricks-only setup (safe no-op outside Databricks)\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "IS_DATABRICKS_RUNTIME = bool(str(os.environ.get(\"DATABRICKS_RUNTIME_VERSION\", \"\")).strip())\n",
        "DBX_NOTEBOOK_PATH = \"\"\n",
        "DBX_REPO_SRC_PATH = \"\"\n",
        "DBX_REPOS_ROOT = Path(\"/Workspace/Repos\")\n",
        "\n",
        "try:\n",
        "    notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()  # type: ignore[name-defined]\n",
        "    DBX_NOTEBOOK_PATH = str(notebook_path)\n",
        "    IS_DATABRICKS_RUNTIME = True\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "if DBX_NOTEBOOK_PATH and \"/notebooks/\" in DBX_NOTEBOOK_PATH:\n",
        "    DBX_REPO_SRC_PATH = \"/Workspace\" + DBX_NOTEBOOK_PATH.split(\"/notebooks/\", 1)[0] + \"/src\"\n",
        "\n",
        "if IS_DATABRICKS_RUNTIME:\n",
        "    print(\"[databricks-only] Runtime context detected.\")\n",
        "    if DBX_NOTEBOOK_PATH:\n",
        "        print(f\"[databricks-only] notebook_path={DBX_NOTEBOOK_PATH}\")\n",
        "    if DBX_REPO_SRC_PATH:\n",
        "        print(f\"[databricks-only] repo_src_path={DBX_REPO_SRC_PATH}\")\n",
        "else:\n",
        "    print(\"[databricks-only] Not running on Databricks; this cell is a no-op.\")\n"
      ],
      "id": "8d07b087"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-06T17:14:16.580889Z",
          "start_time": "2026-02-06T17:14:14.135615Z"
        }
      },
      "source": [
        "#######################################################################################################\n",
        "\n",
        "###### Dependency Preflight (Fail Fast)                                                           ######\n",
        "\n",
        "#######################################################################################################\n",
        "\n",
        "\n",
        "import importlib\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def _candidate_src_paths():\n",
        "    candidates = []\n",
        "\n",
        "    # Optional explicit override.\n",
        "    env_src = os.environ.get(\"NATURALIST_COMPANION_SRC\", \"\").strip()\n",
        "    if env_src:\n",
        "        candidates.append(Path(env_src))\n",
        "\n",
        "    # Databricks-only overrides are centralized in the Databricks-only setup cell.\n",
        "    dbx_repo_src = str(globals().get(\"DBX_REPO_SRC_PATH\", \"\") or \"\").strip()\n",
        "    if dbx_repo_src:\n",
        "        candidates.append(Path(dbx_repo_src))\n",
        "\n",
        "    cwd = Path.cwd()\n",
        "    candidates.extend([\n",
        "        cwd / \"src\",\n",
        "        cwd.parent / \"src\",\n",
        "        cwd.parent.parent / \"src\",\n",
        "    ])\n",
        "\n",
        "    # Optional scan for repository-style checkouts.\n",
        "    repos_root = globals().get(\"DBX_REPOS_ROOT\", Path(\"/Workspace/Repos\"))\n",
        "    repos_root = Path(str(repos_root))\n",
        "    if repos_root.exists():\n",
        "        for pkg_dir in repos_root.glob(\"*/*/src/naturalist_companion\"):\n",
        "            candidates.append(pkg_dir.parent)\n",
        "\n",
        "    deduped = []\n",
        "    seen = set()\n",
        "    for item in candidates:\n",
        "        key = str(item)\n",
        "        if key in seen:\n",
        "            continue\n",
        "        seen.add(key)\n",
        "        deduped.append(item)\n",
        "    return deduped\n",
        "\n",
        "\n",
        "for src_path in _candidate_src_paths():\n",
        "    if (src_path / \"naturalist_companion\").exists() and str(src_path) not in sys.path:\n",
        "        sys.path.insert(0, str(src_path))\n",
        "        break\n",
        "\n",
        "\n",
        "CHECKS = [('FAISS backend', ['faiss']), ('LangGraph runtime', ['langgraph']), ('Wikipedia loader module', ['langchain_community.document_loaders', 'langchain.document_loaders']), ('Text splitter module', ['langchain_text_splitters', 'langchain.text_splitter']), ('LangChain vectorstore module', ['langchain_community.vectorstores']), ('LangChain in-memory docstore module', ['langchain_community.docstore.in_memory']), ('Vertex integration', ['langchain_google_vertexai']), ('Google AI Platform SDK', ['google.cloud.aiplatform']), ('Naturalist stategraph module', ['naturalist_companion.stategraph_shared'])]\n",
        "resolved = {}\n",
        "missing = []\n",
        "\n",
        "for label, module_candidates in CHECKS:\n",
        "    matched = None\n",
        "    last_error = None\n",
        "    for module_name in module_candidates:\n",
        "        try:\n",
        "            importlib.import_module(module_name)\n",
        "            matched = module_name\n",
        "            break\n",
        "        except ImportError as import_error:\n",
        "            last_error = f\"{type(import_error).__name__}: {import_error}\"\n",
        "\n",
        "    if matched is not None:\n",
        "        resolved[label] = matched\n",
        "    else:\n",
        "        missing.append((label, module_candidates, last_error))\n",
        "\n",
        "if missing:\n",
        "    non_stategraph_missing = [m for m in missing if m[0] != \"Naturalist stategraph module\"]\n",
        "    if non_stategraph_missing:\n",
        "        missing = non_stategraph_missing\n",
        "\n",
        "    lines = [\"[preflight] Missing required notebook dependencies:\"]\n",
        "    for label, module_candidates, last_error in missing:\n",
        "        lines.append(f\"- {label}: expected one of {', '.join(module_candidates)}\")\n",
        "        if last_error:\n",
        "            lines.append(f\"  last error: {last_error}\")\n",
        "\n",
        "    lines.append(\"\")\n",
        "    lines.append(\"Run the install cell above, restart the kernel, and retry.\")\n",
        "    lines.append(f\"Install hint: {'%pip install -q -r ../requirements-gcp-dev.txt'}\")\n",
        "    lines.append(\"Set NATURALIST_COMPANION_SRC (or DBX_REPO_SRC_PATH in the Databricks-only cell) if needed.\")\n",
        "    raise ModuleNotFoundError(\"\\n\".join(lines))\n",
        "\n",
        "print(\"[preflight] Dependency check passed.\")\n",
        "for label, module_name in resolved.items():\n",
        "    print(f\"  - {label}: {module_name}\")\n"
      ],
      "id": "cd050869ff75eca4",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-06T17:14:16.636218Z",
          "start_time": "2026-02-06T17:14:16.613298Z"
        }
      },
      "source": [
        "#######################################################################################################\n",
        "\n",
        "###### Python Package Imports for this notebook                                                  ######\n",
        "\n",
        "#######################################################################################################\n",
        "\n",
        "\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from typing import Literal\n",
        "from threading import Event, Thread\n",
        "from time import perf_counter\n",
        "\n",
        "from IPython.display import Image, Markdown, display\n",
        "\n",
        "# Mitigate common macOS OpenMP duplicate-library crashes in notebook kernels.\n",
        "os.environ.setdefault(\"KMP_DUPLICATE_LIB_OK\", \"TRUE\")\n",
        "\n",
        "# Silence noisy tqdm widget warning in IDE notebooks when rich progress widgets are unavailable.\n",
        "warnings.filterwarnings(\"ignore\", message=\".*IProgress not found.*\")\n",
        "\n",
        "\n",
        "# LangChain moved WikipediaLoader in newer releases; keep backward compatibility.\n",
        "try:\n",
        "    from langchain_community.document_loaders import WikipediaLoader\n",
        "except ImportError:\n",
        "    from langchain.document_loaders import WikipediaLoader\n",
        "\n",
        "try:\n",
        "    from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "except ImportError:\n",
        "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_google_vertexai import ChatVertexAI, VertexAIEmbeddings\n",
        "\n",
        "\n",
        "def _start_heartbeat(task_label: str, every_s: float = 8.0):\n",
        "    stop = Event()\n",
        "\n",
        "    def _run():\n",
        "        elapsed = 0.0\n",
        "        while not stop.wait(every_s):\n",
        "            elapsed += every_s\n",
        "            print(f\"[{task_label}] still running... {elapsed:.0f}s elapsed\")\n",
        "\n",
        "    thread = Thread(target=_run, daemon=True)\n",
        "    thread.start()\n",
        "    return stop\n",
        "\n",
        "\n",
        "from naturalist_companion.wikipedia_tools import display_wikipedia_images_for_pages\n",
        "\n",
        "\n",
        "STATEGRAPH_AVAILABLE = False\n",
        "_stategraph_import_error = None\n",
        "try:\n",
        "    from naturalist_companion.stategraph_shared import (\n",
        "        build_stategraph_app,\n",
        "        run_i81_eval_harness,\n",
        "        run_stategraph,\n",
        "    )\n",
        "    STATEGRAPH_AVAILABLE = True\n",
        "except Exception as stategraph_import_error:\n",
        "    _stategraph_import_error = stategraph_import_error\n",
        "    print(f\"[stategraph] import error: {type(stategraph_import_error).__name__}: {stategraph_import_error}\")\n",
        "    print(\"[stategraph] StateGraph cells can be skipped until this import works.\")\n",
        "\n",
        "\n",
        "try:\n",
        "    import ipywidgets as _ipywidgets\n",
        "    print(f\"[env] ipywidgets detected: v{_ipywidgets.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"[env] ipywidgets not found in this kernel. Run `%pip install -q ipywidgets` and restart the kernel.\")\n",
        "\n",
        "\n",
        "def _show_vertex_auth_status() -> None:\n",
        "    project = os.environ.get(\"GOOGLE_CLOUD_PROJECT\", os.environ.get(\"GCP_PROJECT\", \"\")).strip()\n",
        "    location = os.environ.get(\"VERTEX_LOCATION\", os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")).strip()\n",
        "    if project:\n",
        "        print(f\"[env] Vertex config detected: project={project}, location={location}\")\n",
        "    else:\n",
        "        print(\"[env] Vertex project is not set. Set GOOGLE_CLOUD_PROJECT (or GCP_PROJECT) before model calls.\")\n",
        "\n",
        "\n",
        "def _vertex_client_kwargs() -> dict:\n",
        "    kwargs = {}\n",
        "    project = os.environ.get(\"GOOGLE_CLOUD_PROJECT\", os.environ.get(\"GCP_PROJECT\", \"\")).strip()\n",
        "    location = os.environ.get(\"VERTEX_LOCATION\", os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")).strip()\n",
        "    if project:\n",
        "        kwargs[\"project\"] = project\n",
        "    if location:\n",
        "        kwargs[\"location\"] = location\n",
        "    return kwargs\n",
        "\n",
        "\n",
        "def _build_vertex_embeddings(model_name: str):\n",
        "    kwargs = _vertex_client_kwargs()\n",
        "    try:\n",
        "        return VertexAIEmbeddings(model_name=model_name, **kwargs)\n",
        "    except TypeError:\n",
        "        return VertexAIEmbeddings(model=model_name, **kwargs)\n",
        "\n",
        "\n",
        "def _build_vertex_chat(model_name: str, temperature: float):\n",
        "    kwargs = _vertex_client_kwargs()\n",
        "    try:\n",
        "        return ChatVertexAI(model_name=model_name, temperature=temperature, **kwargs)\n",
        "    except TypeError:\n",
        "        return ChatVertexAI(model=model_name, temperature=temperature, **kwargs)\n",
        "\n",
        "\n",
        "_show_vertex_auth_status()\n",
        "\n",
        "\n",
        "#######################################################################################################\n",
        "\n",
        "###### Config (Define LLMs, Embeddings, Vector Store, Data Loader specs)                         ######\n",
        "\n",
        "#######################################################################################################\n",
        "\n",
        "\n",
        "# DataLoader Config\n",
        "query_terms = [\n",
        "    \"roadcut\",\n",
        "    \"geology\",\n",
        "    \"sedimentary rock\",\n",
        "    \"stratigraphy\",\n",
        "]\n",
        "max_docs = 12  # Realistic retrieval setting for richer context.\n",
        "\n",
        "# Stage 2 chunking + batching controls (keep small for interactive runs).\n",
        "chunk_size = 1200\n",
        "chunk_overlap = 150\n",
        "embedding_batch_size = 8\n",
        "\n",
        "\n",
        "# Retriever Config\n",
        "k = 4\n",
        "EMBEDDING_MODEL = \"text-embedding-005\"\n",
        "\n",
        "\n",
        "# LLM Config\n",
        "LLM_MODEL = \"gemini-1.5-flash\"\n",
        "TEMPERATURE = 0.25\n",
        "\n",
        "\n",
        "# Response style controls (Roadside Geology audience: curious drivers, practical field learners).\n",
        "RESPONSE_TONE = \"field-guide\"\n",
        "MAX_BULLETS_PER_SECTION = 4\n",
        "\n",
        "\n",
        "# Local artifact settings\n",
        "FAISS_NAMESPACE = \"anc_gcp\"\n",
        "\n",
        "\n",
        "# StateGraph Config\n",
        "STATEGRAPH_PROVIDER: Literal[\"vertex\"] = \"vertex\"\n",
        "STATEGRAPH_LIVE_MAX_DOCS = 16\n",
        "STATEGRAPH_COMMON_CONFIG = {\n",
        "    \"max_retrieval_attempts\": 3,\n",
        "    \"citation_coverage_threshold\": 0.80,\n",
        "    \"runtime_mode\": \"realistic\",\n",
        "    \"llm_temperature\": TEMPERATURE,\n",
        "    \"llm_model\": LLM_MODEL,\n",
        "    \"live_max_docs\": STATEGRAPH_LIVE_MAX_DOCS,\n",
        "}\n",
        "STATEGRAPH_RUN_CONFIG = {\"artifact_root\": \"out/stategraph/notebook_runs\", **STATEGRAPH_COMMON_CONFIG}\n",
        "STATEGRAPH_EVAL_CONFIG = {\"artifact_root\": \"out/stategraph/notebook_eval\", **STATEGRAPH_COMMON_CONFIG}\n"
      ],
      "id": "27b8b525bcb90196",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Query Prompt (Edit This Cell)\n",
        "\n",
        "Use the next code cell to set the active question(s).\n",
        "\n",
        "Question types this notebook is designed for:\n",
        "- Detour geology: legal pull-offs or short walks near a route segment\n",
        "- Safety-first prompts: where to stop and what to avoid roadside\n",
        "- Route constraints: city/exit anchors plus max detour minutes\n",
        "- Beginner field interpretation: what visual clues to look for and why they matter\n",
        "\n",
        "Tip: Include your nearest city or exit and your max detour time to improve stop recommendations.\n"
      ],
      "id": "963f8bf2fdca20f6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-06T17:14:16.641123Z",
          "start_time": "2026-02-06T17:14:16.637187Z"
        }
      },
      "source": [
        "example_question = \"I am on I-81 near Hagerstown with a 30-minute detour. Where can I safely stop to observe folded Valley-and-Ridge strata, and what exactly should I look for?\"\n",
        "\n",
        "example_questions = [\n",
        "    \"I am driving I-81 near Bristol, TN. Give me two legal pull-off stops where I can see clear sedimentary layering, and tell me exactly what to look for.\",\n",
        "    \"Near I-81 between Winchester and Strasburg, where can I safely stop to see Valley-and-Ridge structure, and what field clues confirm folding?\",\n",
        "    \"I have 45 minutes near Hagerstown, MD. What roadside geology stop gives the best payoff for a beginner, and what story does the outcrop tell?\",\n",
        "    \"Along I-81 in the Shenandoah Valley, point me to a short-walk stop to compare rock type and landform, then explain why that match matters.\",\n",
        "    \"On an I-81 drive day, suggest one stop where I can observe evidence of ancient seas or sediment transport, with specific visual clues.\",\n",
        "]\n",
        "\n",
        "# StateGraph run can use the same prompt by default; edit independently if desired.\n",
        "stategraph_question = example_question\n",
        "\n",
        "def _parse_place_query_list(raw: str, max_items: int = 6) -> list[str]:\n",
        "    text = str(raw or \"\").strip()\n",
        "    if not text:\n",
        "        return []\n",
        "    match = re.search(r\"\\[[\\s\\S]*\\]\", text)\n",
        "    if match:\n",
        "        text = match.group(0)\n",
        "    try:\n",
        "        data = json.loads(text)\n",
        "    except json.JSONDecodeError:\n",
        "        data = [ln.strip(\" -â€¢\\t\") for ln in text.splitlines() if ln.strip()]\n",
        "\n",
        "    out: list[str] = []\n",
        "    for item in data if isinstance(data, list) else []:\n",
        "        value = str(item or \"\").strip()\n",
        "        if not value:\n",
        "            continue\n",
        "        low = value.lower()\n",
        "        if any(token in low for token in (\"interstate\", \"highway\", \"i-81\", \"route \")):\n",
        "            continue\n",
        "        if value not in out:\n",
        "            out.append(value)\n",
        "        if len(out) >= int(max_items):\n",
        "            break\n",
        "    return out\n",
        "\n",
        "\n",
        "def _generate_place_image_queries_with_model(question: str, max_items: int = 6) -> list[str]:\n",
        "    prompt = (\n",
        "        \"Return only JSON: an array of concise Wikipedia search queries for NATURAL LANDSCAPES \"\n",
        "        \"near this route question. Exclude highways, interstates, and city-only queries. \"\n",
        "        \"Prefer valleys, ridges, mountains, parks, overlooks, and geologic landforms. \"\n",
        "        f\"Question: {question}\"\n",
        "    )\n",
        "    try:\n",
        "        planner = _build_vertex_chat(LLM_MODEL, min(0.3, float(TEMPERATURE)))\n",
        "        response = planner.invoke(prompt)\n",
        "        raw = getattr(response, \"content\", response)\n",
        "        places = _parse_place_query_list(raw, max_items=max_items)\n",
        "        if places:\n",
        "            return places\n",
        "    except Exception as query_error:\n",
        "        print(f\"[query] place query generation fallback: {type(query_error).__name__}: {query_error}\")\n",
        "\n",
        "    return [\n",
        "        \"Shenandoah Valley overlooks\",\n",
        "        \"Blue Ridge Mountains viewpoints\",\n",
        "        \"Appalachian Valley and Ridge outcrops\",\n",
        "        \"Catoctin Mountain Park geology\",\n",
        "        \"Great North Mountain ridge overlook\",\n",
        "    ][: max(1, int(max_items))]\n",
        "\n",
        "\n",
        "place_image_queries = _generate_place_image_queries_with_model(example_question, max_items=6)\n",
        "print(\"[query] place_image_queries:\")\n",
        "for query_idx, query_text in enumerate(place_image_queries, start=1):\n",
        "    print(f\"  {query_idx}. {query_text}\")\n"
      ],
      "id": "2c2cfe79b5c449c5",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-06T17:14:21.780603Z",
          "start_time": "2026-02-06T17:14:16.641428Z"
        }
      },
      "source": [
        "#######################################################################################################\n",
        "\n",
        "###### Stage 1/3: Wikipedia Data Load                                                            ######\n",
        "\n",
        "#######################################################################################################\n",
        "\n",
        "\n",
        "print(\"[stage 1/3] Starting Wikipedia document load...\")\n",
        "query = \" \".join(query_terms) if isinstance(query_terms, list) else query_terms\n",
        "print(f\"[stage 1/3] query={query!r}, max_docs={max_docs}\")\n",
        "\n",
        "stage1_heartbeat = _start_heartbeat(\"stage 1/3 wikipedia load\", every_s=8.0)\n",
        "t0 = perf_counter()\n",
        "try:\n",
        "    docs = WikipediaLoader(query=query, load_max_docs=max_docs).load()\n",
        "finally:\n",
        "    stage1_heartbeat.set()\n",
        "t1 = perf_counter()\n",
        "\n",
        "print(f\"[stage 1/3] Loaded {len(docs)} document(s) in {t1 - t0:.2f}s\")\n",
        "if not docs:\n",
        "    raise RuntimeError(\"No documents loaded from Wikipedia. Adjust query_terms/max_docs and re-run stage 1.\")\n",
        "\n",
        "print(\"[stage 1/3] Sample titles:\")\n",
        "for sample_idx, sample_doc in enumerate(docs[:3], start=1):\n",
        "    sample_title = str((sample_doc.metadata or {}).get(\"title\") or f\"doc_{sample_idx}\")\n",
        "    sample_source = str((sample_doc.metadata or {}).get(\"source\") or \"n/a\")\n",
        "    print(f\"  {sample_idx}. {sample_title} ({sample_source})\")\n",
        "\n",
        "print(\"[stage 1/3] Wikipedia image previews from loaded pages...\")\n",
        "display_wikipedia_images_for_pages(docs, max_images=min(3, len(docs)))\n",
        "\n",
        "print(\"[stage 1/3] Wikipedia image previews near place queries...\")\n",
        "display_wikipedia_images_for_pages(place_image_queries, max_images=min(5, len(place_image_queries)))\n"
      ],
      "id": "d3ace855a2c925b",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-06T17:14:22.224172Z",
          "start_time": "2026-02-06T17:14:21.790910Z"
        }
      },
      "source": [
        "#######################################################################################################\n",
        "\n",
        "###### Stage 2/3: Build + Save FAISS Index, Then Retrieve                                        ######\n",
        "\n",
        "#######################################################################################################\n",
        "\n",
        "\n",
        "if \"docs\" not in globals() or not docs:\n",
        "    raise RuntimeError(\"`docs` not found. Run Stage 1/3 first.\")\n",
        "\n",
        "print(f\"[stage 2/3] Building embeddings with model={EMBEDDING_MODEL!r} and Vertex config={_vertex_client_kwargs()}...\")\n",
        "print(\n",
        "    f\"[stage 2/3] Chunking docs with chunk_size={chunk_size}, chunk_overlap={chunk_overlap}, \"\n",
        "    f\"embedding_batch_size={embedding_batch_size}\"\n",
        ")\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "split_docs = splitter.split_documents(docs)\n",
        "if not split_docs:\n",
        "    raise RuntimeError(\"Chunking produced 0 documents. Adjust chunk_size/chunk_overlap and retry.\")\n",
        "\n",
        "total_chars = sum(len(str(d.page_content or \"\")) for d in split_docs)\n",
        "print(f\"[stage 2/3] Prepared {len(split_docs)} chunk(s), total_chars={total_chars}\")\n",
        "\n",
        "embeddings = _build_vertex_embeddings(EMBEDDING_MODEL)\n",
        "\n",
        "batch_size = max(1, int(embedding_batch_size))\n",
        "vector_store = None\n",
        "\n",
        "stage2_heartbeat = _start_heartbeat(\"stage 2/3 embedding/index\", every_s=8.0)\n",
        "t2 = perf_counter()\n",
        "try:\n",
        "    for start in range(0, len(split_docs), batch_size):\n",
        "        batch = split_docs[start : start + batch_size]\n",
        "        b0 = perf_counter()\n",
        "        if vector_store is None:\n",
        "            vector_store = FAISS.from_documents(batch, embeddings)\n",
        "        else:\n",
        "            vector_store.add_documents(batch)\n",
        "        b1 = perf_counter()\n",
        "\n",
        "        done = min(start + batch_size, len(split_docs))\n",
        "        pct = (100.0 * done) / len(split_docs)\n",
        "        print(\n",
        "            f\"[stage 2/3] Embedded batch {start // batch_size + 1}: \"\n",
        "            f\"{done}/{len(split_docs)} chunks ({pct:.1f}%) in {b1 - b0:.2f}s\"\n",
        "        )\n",
        "finally:\n",
        "    stage2_heartbeat.set()\n",
        "\n",
        "t3 = perf_counter()\n",
        "if vector_store is None:\n",
        "    raise RuntimeError(\"Vector store was not created.\")\n",
        "\n",
        "print(f\"[stage 2/3] Built FAISS index in {t3 - t2:.2f}s\")\n",
        "\n",
        "\n",
        "faiss_base = os.environ.get(\"ANC_FAISS_DIR\", \"\").strip()\n",
        "if faiss_base:\n",
        "    faiss_dir = (Path(faiss_base).expanduser() / FAISS_NAMESPACE).resolve()\n",
        "else:\n",
        "    faiss_dir = (Path.home() / \"DATA\" / \"naturalist-companion\" / \"faiss\" / FAISS_NAMESPACE).resolve()\n",
        "\n",
        "faiss_dir.mkdir(parents=True, exist_ok=True)\n",
        "vector_store.save_local(str(faiss_dir))\n",
        "print(f\"[stage 2/3] Saved FAISS index to: {faiss_dir}\")\n",
        "\n",
        "\n",
        "print(f\"[stage 2/3] Running similarity search for question={example_question!r}, k={k}...\")\n",
        "results = vector_store.similarity_search(example_question, k=k)\n",
        "print(f\"[stage 2/3] Retrieved {len(results)} result(s)\")\n",
        "\n",
        "for result_idx, result_doc in enumerate(results, start=1):\n",
        "    result_title = str((result_doc.metadata or {}).get(\"title\") or f\"result_{result_idx}\")\n",
        "    result_source = str((result_doc.metadata or {}).get(\"source\") or \"n/a\")\n",
        "    result_snippet = str(result_doc.page_content or \"\")[:220].replace(\"\\n\", \" \")\n",
        "    print(f\"  {result_idx}. {result_title} ({result_source})\")\n",
        "    print(f\"     {result_snippet}...\")\n",
        "\n",
        "print(\"[stage 2/3] Wikipedia image previews from retrieved pages...\")\n",
        "display_wikipedia_images_for_pages(results, max_images=min(4, len(results)))\n"
      ],
      "id": "675e408e7a777486",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-06T17:14:32.640009Z",
          "start_time": "2026-02-06T17:14:22.226034Z"
        }
      },
      "source": [
        "#######################################################################################################\n",
        "\n",
        "###### Stage 3/3: Generate Answer with ChatVertexAI                                                ######\n",
        "\n",
        "#######################################################################################################\n",
        "\n",
        "\n",
        "if \"vector_store\" not in globals():\n",
        "    raise RuntimeError(\"`vector_store` not found. Run Stage 2/3 first.\")\n",
        "\n",
        "print(f\"[stage 3/3] Generating answer with model={LLM_MODEL!r} and Vertex config={_vertex_client_kwargs()}...\")\n",
        "llm = _build_vertex_chat(LLM_MODEL, TEMPERATURE)\n",
        "\n",
        "voice_instructions = f\"\"\"\n",
        "You are writing in a concise Roadside Geology field-guide voice for curious drivers.\n",
        "Tone:\n",
        "- Plainspoken, observant, and practical (not academic).\n",
        "- Emphasize what can be seen from legal/safe pull-offs or short walks.\n",
        "- Explain key geology in everyday language, then add one precise term when useful.\n",
        "- Include safety and access realism (do not suggest unsafe roadside behavior).\n",
        "Output format:\n",
        "1) \"Where to stop\" (up to {MAX_BULLETS_PER_SECTION} bullets)\n",
        "2) \"What to look for\" (up to {MAX_BULLETS_PER_SECTION} bullets)\n",
        "3) \"Why it matters\" (2-4 sentences)\n",
        "4) \"Citations\" (Wikipedia URLs only)\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "def _context_for_question(question: str, top_k: int = 2) -> str:\n",
        "    local_results = vector_store.similarity_search(question, k=max(1, top_k))\n",
        "    context_lines: list[str] = []\n",
        "    for context_idx, context_doc in enumerate(local_results, start=1):\n",
        "        context_title = str((context_doc.metadata or {}).get(\"title\") or f\"result_{context_idx}\")\n",
        "        context_source = str((context_doc.metadata or {}).get(\"source\") or \"n/a\")\n",
        "        context_snippet = str(context_doc.page_content or \"\")[:450].replace(\"\\n\", \" \")\n",
        "        context_lines.append(f\"[{context_idx}] {context_title} ({context_source}) :: {context_snippet}\")\n",
        "    return \"\\n\".join(context_lines)\n",
        "\n",
        "\n",
        "def answer_question(question: str) -> str:\n",
        "    context_block = _context_for_question(question, top_k=max(1, k))\n",
        "    prompt = (\n",
        "        f\"Use only the provided Wikipedia-grounded context when you can.\\n\\n\"\n",
        "        f\"Question: {question}\\n\\n\"\n",
        "        f\"Context:\\n{context_block}\\n\\n\"\n",
        "        f\"Style requirements:\\n{voice_instructions}\"\n",
        "    )\n",
        "\n",
        "    llm_heartbeat = _start_heartbeat(\"stage 3/3 llm\", every_s=8.0)\n",
        "    started_at = perf_counter()\n",
        "    llm_response = None\n",
        "    try:\n",
        "        llm_response = llm.invoke(prompt)\n",
        "    finally:\n",
        "        llm_heartbeat.set()\n",
        "    dt = perf_counter() - started_at\n",
        "    print(f\"[stage 3/3] LLM response received in {dt:.2f}s\")\n",
        "    if llm_response is None:\n",
        "        raise RuntimeError(\"LLM did not return a response.\")\n",
        "    response_content = getattr(llm_response, \"content\", llm_response)\n",
        "    return str(response_content)\n",
        "\n",
        "\n",
        "print(f\"[stage 3/3] Primary question:\\n- {example_question}\")\n",
        "primary_answer = answer_question(example_question)\n",
        "print(\"\\nAnswer:\\n\")\n",
        "print(primary_answer)\n"
      ],
      "id": "bbeb769fabb043f0",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-06T17:15:15.443151Z",
          "start_time": "2026-02-06T17:14:32.647767Z"
        }
      },
      "source": [
        "#######################################################################################################\n",
        "\n",
        "###### Stage 3b/3: Run All Example Questions                                                     ######\n",
        "\n",
        "#######################################################################################################\n",
        "\n",
        "\n",
        "if \"answer_question\" not in globals():\n",
        "    raise RuntimeError(\"`answer_question` not found. Run Stage 3/3 first.\")\n",
        "\n",
        "if \"example_questions\" not in globals() or not example_questions:\n",
        "    raise RuntimeError(\"`example_questions` is empty. Check config cell.\")\n",
        "\n",
        "all_answers = []\n",
        "print(f\"[stage 3b/3] Running {len(example_questions)} example question(s)...\")\n",
        "\n",
        "for question_idx, question_text in enumerate(example_questions, start=1):\n",
        "    print(\"\\n\" + \"=\" * 110)\n",
        "    print(f\"[stage 3b/3] Question {question_idx}/{len(example_questions)}\")\n",
        "    print(question_text)\n",
        "    print(\"=\" * 110)\n",
        "\n",
        "    answer_text = answer_question(question_text)\n",
        "    all_answers.append({\"question\": question_text, \"answer\": answer_text})\n",
        "\n",
        "    print(\"\\nResponse:\\n\")\n",
        "    print(answer_text)\n",
        "\n",
        "print(f\"\\n[stage 3b/3] Completed {len(all_answers)} question(s).\")\n"
      ],
      "id": "aba8420e5ae6bf0d",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Canonical Workflow Diagram (StateGraph)\n",
        "\n",
        "StateGraph is the canonical workflow diagram for this notebook because it reflects the shared orchestration logic and is less likely to drift than a separate static PlantUML drawing.\n"
      ],
      "id": "4f8174e829845b78"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-06T17:15:15.460740Z",
          "start_time": "2026-02-06T17:15:15.450748Z"
        }
      },
      "source": [
        "# Uncomment in fresh environments:\n",
        "# %pip install -q -r ../requirements-gcp-dev.txt\n",
        "\n",
        "if STATEGRAPH_AVAILABLE:\n",
        "    print(\"[stategraph] Ready: naturalist_companion.stategraph_shared imported in setup cells.\")\n",
        "else:\n",
        "    print(\"[stategraph] Unavailable: fix setup/import paths above; diagram/run/eval cells will skip.\")\n"
      ],
      "id": "d7432ecf7be1c970",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-06T17:15:15.546386Z",
          "start_time": "2026-02-06T17:15:15.462732Z"
        }
      },
      "source": [
        "if not STATEGRAPH_AVAILABLE:\n",
        "    print(\"[stategraph] Skipping canonical diagram: naturalist_companion not available.\")\n",
        "else:\n",
        "    provider: Literal[\"vertex\"] = STATEGRAPH_PROVIDER\n",
        "    app = build_stategraph_app(provider=provider)\n",
        "    print('Compiled StateGraph successfully for provider:', provider)\n",
        "\n",
        "    # Render a real image (PNG bytes) instead of plain Mermaid text.\n",
        "    try:\n",
        "        png_bytes = app.get_graph().draw_mermaid_png()\n",
        "        display(Image(data=png_bytes, width=880))\n",
        "    except Exception as graph_render_error:\n",
        "        display(Markdown(f'Graph render fallback (text). Error: `{type(graph_render_error).__name__}: {graph_render_error}`'))\n",
        "        print(app.get_graph().draw_mermaid())\n"
      ],
      "id": "1b0898602a15f1fe",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-06T17:15:15.864395Z",
          "start_time": "2026-02-06T17:15:15.549202Z"
        }
      },
      "source": [
        "if not STATEGRAPH_AVAILABLE:\n",
        "    print(\"[stategraph] Skipping run_stategraph: naturalist_companion not available.\")\n",
        "else:\n",
        "    result = run_stategraph(\n",
        "        stategraph_question,\n",
        "        provider=STATEGRAPH_PROVIDER,\n",
        "        config=STATEGRAPH_RUN_CONFIG,\n",
        "    )\n",
        "    final_output = result['final_output']\n",
        "    print('Question:', stategraph_question)\n",
        "    print('Provider:', final_output['provider'])\n",
        "    print('Route:', final_output['route_decision']['decision'])\n",
        "    print('Quality passed:', final_output['quality']['passed'])\n",
        "    quality_reasons = final_output['quality'].get('reasons', [])\n",
        "    print('Quality reasons:', ', '.join(quality_reasons) if quality_reasons else 'none')\n",
        "    print('Attempts:', final_output['retrieval_attempts'])\n",
        "    print('Artifact dir:', result['artifact_dir'])\n",
        "    print('Response:')\n",
        "    print(final_output['answer']['response'])\n",
        "    print('Citation image previews:')\n",
        "    display_wikipedia_images_for_pages(final_output['answer'].get('citations', []), max_images=4)\n"
      ],
      "id": "d114d6ac54f134e3",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-06T17:15:16.058461Z",
          "start_time": "2026-02-06T17:15:15.865147Z"
        }
      },
      "source": [
        "if not STATEGRAPH_AVAILABLE:\n",
        "    print(\"[stategraph] Skipping eval harness: naturalist_companion not available.\")\n",
        "else:\n",
        "    report = run_i81_eval_harness(provider=STATEGRAPH_PROVIDER, config=STATEGRAPH_EVAL_CONFIG)\n",
        "    print(report['summary'])\n",
        "    print(report['artifact_root'])\n"
      ],
      "id": "6d727da8ec2791f1",
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
