{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANC StateGraph (Vertex AI Mode)\n",
    "\n",
    "This notebook uses the same StateGraph orchestration as the Ollama notebook, with provider mode switched.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################\n",
    "\n",
    "###### Dependency Preflight (Fail Fast)                                                           ######\n",
    "\n",
    "#######################################################################################################\n",
    "\n",
    "\n",
    "import importlib\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def _candidate_src_paths():\n",
    "    candidates = []\n",
    "\n",
    "    # Optional explicit override.\n",
    "    env_src = os.environ.get(\"NATURALIST_COMPANION_SRC\", \"\").strip()\n",
    "    if env_src:\n",
    "        candidates.append(Path(env_src))\n",
    "\n",
    "    # Databricks notebook context path (when available).\n",
    "    try:\n",
    "        notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "        if \"/notebooks/\" in notebook_path:\n",
    "            repo_workspace_path = \"/Workspace\" + notebook_path.split(\"/notebooks/\", 1)[0]\n",
    "            candidates.append(Path(repo_workspace_path) / \"src\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    cwd = Path.cwd()\n",
    "    candidates.extend([\n",
    "        cwd / \"src\",\n",
    "        cwd.parent / \"src\",\n",
    "        cwd.parent.parent / \"src\",\n",
    "    ])\n",
    "\n",
    "    repos_root = Path(\"/Workspace/Repos\")\n",
    "    if repos_root.exists():\n",
    "        for pkg_dir in repos_root.glob(\"*/*/src/naturalist_companion\"):\n",
    "            candidates.append(pkg_dir.parent)\n",
    "\n",
    "    deduped = []\n",
    "    seen = set()\n",
    "    for item in candidates:\n",
    "        key = str(item)\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        deduped.append(item)\n",
    "    return deduped\n",
    "\n",
    "\n",
    "for src_path in _candidate_src_paths():\n",
    "    if (src_path / \"naturalist_companion\").exists() and str(src_path) not in sys.path:\n",
    "        sys.path.insert(0, str(src_path))\n",
    "        break\n",
    "\n",
    "\n",
    "CHECKS = [('FAISS backend', ['faiss']), ('Wikipedia loader module', ['langchain_community.document_loaders', 'langchain.document_loaders']), ('Text splitter module', ['langchain_text_splitters', 'langchain.text_splitter']), ('LangChain vectorstore module', ['langchain_community.vectorstores']), ('LangChain in-memory docstore module', ['langchain_community.docstore.in_memory']), ('Vertex integration', ['langchain_google_vertexai']), ('Google AI Platform SDK', ['google.cloud.aiplatform']), ('Naturalist stategraph module', ['naturalist_companion.stategraph_shared'])]\n",
    "resolved = {}\n",
    "missing = []\n",
    "\n",
    "for label, module_candidates in CHECKS:\n",
    "    matched = None\n",
    "    last_error = None\n",
    "    for module_name in module_candidates:\n",
    "        try:\n",
    "            importlib.import_module(module_name)\n",
    "            matched = module_name\n",
    "            break\n",
    "        except Exception as exc:\n",
    "            last_error = f\"{type(exc).__name__}: {exc}\"\n",
    "\n",
    "    if matched is not None:\n",
    "        resolved[label] = matched\n",
    "    else:\n",
    "        missing.append((label, module_candidates, last_error))\n",
    "\n",
    "if missing:\n",
    "    lines = [\"[preflight] Missing required notebook dependencies:\"]\n",
    "    for label, module_candidates, last_error in missing:\n",
    "        lines.append(f\"- {label}: expected one of {', '.join(module_candidates)}\")\n",
    "        if last_error:\n",
    "            lines.append(f\"  last error: {last_error}\")\n",
    "\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"Run the install cell above, restart the kernel, and retry.\")\n",
    "    lines.append(f\"Install hint: {'%pip install -q -r ../requirements-gcp-dev.txt'}\")\n",
    "    lines.append(\"If this repo is synced in Databricks, set NATURALIST_COMPANION_SRC to your repo src path if needed.\")\n",
    "    raise ModuleNotFoundError(\"\\n\".join(lines))\n",
    "\n",
    "print(\"[preflight] Dependency check passed.\")\n",
    "for label, module_name in resolved.items():\n",
    "    print(f\"  - {label}: {module_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from naturalist_companion.stategraph_shared import run_i81_eval_harness, run_stategraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provider = 'vertex'\n",
    "question = 'I am on I-81 near Hagerstown. What geology should I look for?'\n",
    "result = run_stategraph(question, provider='vertex', config={'artifact_root': 'out/stategraph/notebook_runs'})\n",
    "print(result['final_output']['provider'])\n",
    "print(result['final_output']['route_decision'])\n",
    "print(result['final_output']['quality'])\n",
    "print(result['artifact_dir'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = run_i81_eval_harness(provider='vertex', config={'artifact_root': 'out/stategraph/notebook_eval'})\n",
    "print(report['summary'])\n",
    "print(report['artifact_root'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
