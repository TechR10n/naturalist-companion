{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANC Databricks: Wikipedia + FAISS + Databricks + StateGraph\n",
    "\n",
    "This notebook combines two Databricks workflows in one place:\n",
    "\n",
    "- Baseline notebook pipeline: Wikipedia -> chunking -> FAISS -> Databricks-hosted Q&A with `ChatDatabricks`\n",
    "- Canonical orchestration diagram + run flow via shared `naturalist_companion.stategraph_shared`\n",
    "\n",
    "Provider mapping used here:\n",
    "- `ChatOllama` -> `ChatDatabricks`\n",
    "- `OllamaEmbeddings` -> `DatabricksEmbeddings`\n",
    "- Same `WikipediaLoader` + `FAISS` retrieval flow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup (Install -> Restart -> Check)\n",
    "\n",
    "Run this setup section top-to-bottom once. It is intentionally split into small steps so it is obvious which libraries are being installed.\n",
    "\n",
    "1. Setup Step 1/7: Install core libraries (`backoff`, `databricks-langchain`, `mlflow-skinny[databricks]`, `langchain*`, `langgraph`, `faiss-cpu`, `wikipedia`).\n",
    "2. Setup Step 2/7: Restart kernel.\n",
    "3. Setup Step 3/7: Verify core imports.\n",
    "4. Setup Step 4/7: Install widget libraries (`ipywidgets`, `jupyterlab_widgets`).\n",
    "5. Setup Step 5/7: Restart kernel.\n",
    "6. Setup Step 6/7: Verify widget imports.\n",
    "7. Setup Step 7/7: Run final preflight (dependencies + repo path checks).\n",
    "\n",
    "### Databricks auth prerequisites\n",
    "\n",
    "- Create/sign in to your Databricks Free Edition workspace.\n",
    "- Create a Personal Access Token (PAT) in Databricks.\n",
    "- For local notebook runs, set:\n",
    "  - `export DATABRICKS_HOST=\"https://<your-workspace-host>\"`\n",
    "  - `export DATABRICKS_TOKEN=\"<your-pat>\"`\n",
    "- Ensure your user/token has `Can Query` permission for the selected model endpoints.\n",
    "\n",
    "### Foundation model endpoints\n",
    "\n",
    "This notebook defaults to Databricks-hosted Foundation Model API endpoints:\n",
    "- Embeddings: `databricks-bge-large-en`\n",
    "- Chat: `databricks-meta-llama-3-1-8b-instruct`\n",
    "\n",
    "If your workspace exposes different endpoint names, override with:\n",
    "- `DATABRICKS_EMBEDDING_ENDPOINT`\n",
    "- `DATABRICKS_LLM_ENDPOINT`\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#######################################################################################################\n",
    "\n",
    "###### Setup Step 1/7: Install Core Libraries                                                       ######\n",
    "\n",
    "#######################################################################################################\n",
    "\n",
    "\n",
    "import importlib.util\n",
    "import shlex\n",
    "from pathlib import Path\n",
    "from IPython import get_ipython\n",
    "\n",
    "\n",
    "core_packages = [\n",
    "    \"backoff==2.2.1\",\n",
    "    \"databricks-langchain\",\n",
    "    \"mlflow-skinny[databricks]\",\n",
    "    \"langchain==0.3.27\",\n",
    "    \"langchain-core==0.3.83\",\n",
    "    \"langchain-community==0.3.31\",\n",
    "    \"langchain-text-splitters==0.3.11\",\n",
    "    \"langgraph==0.5.3\",\n",
    "    \"faiss-cpu==1.13.2\",\n",
    "    \"wikipedia==1.4.0\",\n",
    "]\n",
    "core_module_checks = [\n",
    "    \"faiss\",\n",
    "    \"langgraph\",\n",
    "    \"databricks_langchain\",\n",
    "    \"langchain_community\",\n",
    "    \"langchain_text_splitters\",\n",
    "    \"wikipedia\",\n",
    "]\n",
    "\n",
    "requirements_file = Path(\"../requirements-dbrx-dev.txt\")\n",
    "use_requirements_file_for_core = False  # Set True to install from ../requirements-dbrx-dev.txt.\n",
    "force_reinstall_core = False\n",
    "\n",
    "\n",
    "def _flatten_requirements(path: Path, seen=None):\n",
    "    seen = set() if seen is None else seen\n",
    "    resolved = path.resolve()\n",
    "    key = str(resolved)\n",
    "    if key in seen:\n",
    "        return []\n",
    "    seen.add(key)\n",
    "\n",
    "    if not resolved.exists():\n",
    "        return []\n",
    "\n",
    "    packages = []\n",
    "    for raw_line in resolved.read_text().splitlines():\n",
    "        line = raw_line.strip()\n",
    "        if not line or line.startswith(\"#\"):\n",
    "            continue\n",
    "        if line.startswith(\"-r \"):\n",
    "            nested = (resolved.parent / line.split(maxsplit=1)[1]).resolve()\n",
    "            packages.extend(_flatten_requirements(nested, seen))\n",
    "            continue\n",
    "        packages.append(line)\n",
    "    return packages\n",
    "\n",
    "\n",
    "missing_core_modules = [name for name in core_module_checks if importlib.util.find_spec(name) is None]\n",
    "\n",
    "if not missing_core_modules and not force_reinstall_core:\n",
    "    print(\"[setup step 1/7] Core libraries are already available in this kernel. Skipping install.\")\n",
    "else:\n",
    "    if missing_core_modules:\n",
    "        print(f\"[setup step 1/7] Missing core modules: {', '.join(missing_core_modules)}\")\n",
    "    if force_reinstall_core:\n",
    "        print(\"[setup step 1/7] force_reinstall_core=True, running install anyway.\")\n",
    "\n",
    "    if use_requirements_file_for_core and requirements_file.exists():\n",
    "        packages_to_show = _flatten_requirements(requirements_file)\n",
    "        print(f\"[setup step 1/7] Installing from requirements file: {requirements_file}\")\n",
    "        pip_cmd = f\"install -q -U -r {requirements_file}\"\n",
    "    else:\n",
    "        packages_to_show = core_packages\n",
    "        print(\"[setup step 1/7] Installing from explicit package list.\")\n",
    "        pip_cmd = \"install -q -U \" + \" \".join(shlex.quote(pkg) for pkg in core_packages)\n",
    "\n",
    "    print(\"[setup step 1/7] Libraries that will be installed:\")\n",
    "    for pkg in packages_to_show:\n",
    "        print(f\"  - {pkg}\")\n",
    "\n",
    "    ip = get_ipython()\n",
    "    if ip is None:\n",
    "        raise RuntimeError(\"IPython kernel not found; cannot run %pip install.\")\n",
    "\n",
    "    ip.run_line_magic(\"pip\", pip_cmd)\n",
    "    print(\"[setup step 1/7] Core install command finished.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#######################################################################################################\n",
    "\n",
    "###### Setup Step 2/7: Restart Kernel After Core Install                                            ######\n",
    "\n",
    "#######################################################################################################\n",
    "\n",
    "\n",
    "RESTART_NOW = False  # Set True to trigger restart in Databricks.\n",
    "\n",
    "if RESTART_NOW:\n",
    "    try:\n",
    "        dbutils.library.restartPython()\n",
    "    except NameError as exc:\n",
    "        raise RuntimeError(\n",
    "            \"Automatic restart is available in Databricks only. Use your IDE/Jupyter restart action.\"\n",
    "        ) from exc\n",
    "    except Exception as exc:\n",
    "        raise RuntimeError(f\"Automatic restart failed: {type(exc).__name__}: {exc}\") from exc\n",
    "else:\n",
    "    print(\"[setup step 2/7] Restart the kernel now.\")\n",
    "    print(\"  - Databricks: set RESTART_NOW=True and rerun this cell (or restart from the UI).\")\n",
    "    print(\"  - VS Code/Jupyter: use Restart Kernel.\")\n",
    "    print(\"After restarting, run Setup Step 3/7.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#######################################################################################################\n",
    "\n",
    "###### Setup Step 3/7: Verify Core Imports                                                         ######\n",
    "\n",
    "#######################################################################################################\n",
    "\n",
    "\n",
    "import importlib\n",
    "\n",
    "\n",
    "checks = [\n",
    "    (\"FAISS backend\", [\"faiss\"]),\n",
    "    (\"LangGraph runtime\", [\"langgraph\"]),\n",
    "    (\"Databricks integration\", [\"databricks_langchain\"]),\n",
    "    (\"Wikipedia loader import\", [\"langchain_community.document_loaders\", \"langchain.document_loaders\"]),\n",
    "    (\"Text splitter import\", [\"langchain_text_splitters\", \"langchain.text_splitter\"]),\n",
    "    (\"Vectorstore import\", [\"langchain_community.vectorstores\"]),\n",
    "    (\"In-memory docstore import\", [\"langchain_community.docstore.in_memory\"]),\n",
    "]\n",
    "\n",
    "resolved = {}\n",
    "missing = []\n",
    "\n",
    "for label, module_candidates in checks:\n",
    "    matched = None\n",
    "    for module_name in module_candidates:\n",
    "        try:\n",
    "            importlib.import_module(module_name)\n",
    "            matched = module_name\n",
    "            break\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if matched is None:\n",
    "        missing.append((label, module_candidates))\n",
    "    else:\n",
    "        resolved[label] = matched\n",
    "\n",
    "if missing:\n",
    "    lines = [\"[setup step 3/7] Core import checks failed:\"]\n",
    "    for label, module_candidates in missing:\n",
    "        lines.append(f\"- {label}: expected one of {', '.join(module_candidates)}\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"Re-run setup step 1/7, then restart kernel (step 2/7), then run this check again.\")\n",
    "    raise ModuleNotFoundError(\"\\n\".join(lines))\n",
    "\n",
    "print(\"[setup step 3/7] Core imports verified.\")\n",
    "for label, module_name in resolved.items():\n",
    "    print(f\"  - {label}: {module_name}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#######################################################################################################\n",
    "\n",
    "###### Setup Step 4/7: Install Widget Libraries                                                     ######\n",
    "\n",
    "#######################################################################################################\n",
    "\n",
    "\n",
    "import importlib.util\n",
    "from IPython import get_ipython\n",
    "import shlex\n",
    "\n",
    "\n",
    "widget_packages = [\"ipywidgets\", \"jupyterlab_widgets\"]\n",
    "widget_module_checks = [\"ipywidgets\", \"jupyterlab_widgets\"]\n",
    "\n",
    "missing = [name for name in widget_module_checks if importlib.util.find_spec(name) is None]\n",
    "if not missing:\n",
    "    print(\"[setup step 4/7] Widget libraries are already available in this kernel.\")\n",
    "else:\n",
    "    print(\"[setup step 4/7] Installing widget libraries:\")\n",
    "    for pkg in widget_packages:\n",
    "        print(f\"  - {pkg}\")\n",
    "\n",
    "    pip_cmd = \"install -q -U \" + \" \".join(shlex.quote(pkg) for pkg in widget_packages)\n",
    "    ip = get_ipython()\n",
    "    if ip is None:\n",
    "        raise RuntimeError(\"IPython kernel not found; cannot run %pip install.\")\n",
    "\n",
    "    ip.run_line_magic(\"pip\", pip_cmd)\n",
    "    print(\"[setup step 4/7] Widget install command finished.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#######################################################################################################\n",
    "\n",
    "###### Setup Step 5/7: Restart Kernel After Widget Install                                          ######\n",
    "\n",
    "#######################################################################################################\n",
    "\n",
    "\n",
    "RESTART_NOW = False  # Set True to trigger restart in Databricks.\n",
    "\n",
    "if RESTART_NOW:\n",
    "    try:\n",
    "        dbutils.library.restartPython()\n",
    "    except NameError as exc:\n",
    "        raise RuntimeError(\n",
    "            \"Automatic restart is available in Databricks only. Use your IDE/Jupyter restart action.\"\n",
    "        ) from exc\n",
    "    except Exception as exc:\n",
    "        raise RuntimeError(f\"Automatic restart failed: {type(exc).__name__}: {exc}\") from exc\n",
    "else:\n",
    "    print(\"[setup step 5/7] Restart the kernel now.\")\n",
    "    print(\"  - Databricks: set RESTART_NOW=True and rerun this cell (or restart from the UI).\")\n",
    "    print(\"  - VS Code/Jupyter: use Restart Kernel.\")\n",
    "    print(\"After restarting, run Setup Step 6/7.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#######################################################################################################\n",
    "\n",
    "###### Setup Step 6/7: Verify Widget Imports                                                       ######\n",
    "\n",
    "#######################################################################################################\n",
    "\n",
    "\n",
    "import importlib\n",
    "\n",
    "\n",
    "checks = [\n",
    "    (\"ipywidgets\", [\"ipywidgets\"]),\n",
    "    (\"jupyterlab_widgets\", [\"jupyterlab_widgets\"]),\n",
    "]\n",
    "\n",
    "resolved = {}\n",
    "missing = []\n",
    "\n",
    "for label, module_candidates in checks:\n",
    "    matched = None\n",
    "    for module_name in module_candidates:\n",
    "        try:\n",
    "            importlib.import_module(module_name)\n",
    "            matched = module_name\n",
    "            break\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if matched is None:\n",
    "        missing.append((label, module_candidates))\n",
    "    else:\n",
    "        resolved[label] = matched\n",
    "\n",
    "if missing:\n",
    "    lines = [\"[setup step 6/7] Widget import checks failed:\"]\n",
    "    for label, module_candidates in missing:\n",
    "        lines.append(f\"- {label}: expected one of {', '.join(module_candidates)}\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"Re-run setup step 4/7, restart kernel (step 5/7), then run this check again.\")\n",
    "    raise ModuleNotFoundError(\"\\n\".join(lines))\n",
    "\n",
    "print(\"[setup step 6/7] Widget imports verified.\")\n",
    "for label, module_name in resolved.items():\n",
    "    print(f\"  - {label}: {module_name}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#######################################################################################################\n",
    "\n",
    "###### Setup Step 7/7: Final Preflight (Dependencies + Repo Path)                                  ######\n",
    "\n",
    "#######################################################################################################\n",
    "\n",
    "\n",
    "import importlib\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def _candidate_src_paths():\n",
    "    candidates = []\n",
    "\n",
    "    # Optional explicit override.\n",
    "    env_src = os.environ.get(\"NATURALIST_COMPANION_SRC\", \"\").strip()\n",
    "    if env_src:\n",
    "        candidates.append(Path(env_src))\n",
    "\n",
    "    # Databricks notebook context path (when available).\n",
    "    try:\n",
    "        notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "        if \"/notebooks/\" in notebook_path:\n",
    "            repo_workspace_path = \"/Workspace\" + notebook_path.split(\"/notebooks/\", 1)[0]\n",
    "            candidates.append(Path(repo_workspace_path) / \"src\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    cwd = Path.cwd()\n",
    "    candidates.extend([\n",
    "        cwd / \"src\",\n",
    "        cwd.parent / \"src\",\n",
    "        cwd.parent.parent / \"src\",\n",
    "    ])\n",
    "\n",
    "    repos_root = Path(\"/Workspace/Repos\")\n",
    "    if repos_root.exists():\n",
    "        for pkg_dir in repos_root.glob(\"*/*/src/naturalist_companion\"):\n",
    "            candidates.append(pkg_dir.parent)\n",
    "\n",
    "    deduped = []\n",
    "    seen = set()\n",
    "    for item in candidates:\n",
    "        key = str(item)\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        deduped.append(item)\n",
    "    return deduped\n",
    "\n",
    "\n",
    "for src_path in _candidate_src_paths():\n",
    "    if (src_path / \"naturalist_companion\").exists() and str(src_path) not in sys.path:\n",
    "        sys.path.insert(0, str(src_path))\n",
    "        break\n",
    "\n",
    "\n",
    "checks = [\n",
    "    (\"FAISS backend\", [\"faiss\"]),\n",
    "    (\"LangGraph runtime\", [\"langgraph\"]),\n",
    "    (\"Wikipedia loader module\", [\"langchain_community.document_loaders\", \"langchain.document_loaders\"]),\n",
    "    (\"Text splitter module\", [\"langchain_text_splitters\", \"langchain.text_splitter\"]),\n",
    "    (\"LangChain vectorstore module\", [\"langchain_community.vectorstores\"]),\n",
    "    (\"LangChain in-memory docstore module\", [\"langchain_community.docstore.in_memory\"]),\n",
    "    (\"Databricks integration\", [\"databricks_langchain\"]),\n",
    "    (\"Naturalist stategraph module\", [\"naturalist_companion.stategraph_shared\"]),\n",
    "]\n",
    "\n",
    "resolved = {}\n",
    "missing = []\n",
    "\n",
    "for label, module_candidates in checks:\n",
    "    matched = None\n",
    "    last_error = None\n",
    "    for module_name in module_candidates:\n",
    "        try:\n",
    "            importlib.import_module(module_name)\n",
    "            matched = module_name\n",
    "            break\n",
    "        except Exception as exc:\n",
    "            last_error = f\"{type(exc).__name__}: {exc}\"\n",
    "\n",
    "    if matched is not None:\n",
    "        resolved[label] = matched\n",
    "    else:\n",
    "        missing.append((label, module_candidates, last_error))\n",
    "\n",
    "if missing:\n",
    "    non_stategraph_missing = [m for m in missing if m[0] != \"Naturalist stategraph module\"]\n",
    "    if non_stategraph_missing:\n",
    "        missing = non_stategraph_missing\n",
    "\n",
    "    lines = [\"[setup step 7/7] Missing required notebook dependencies:\"]\n",
    "    for label, module_candidates, last_error in missing:\n",
    "        lines.append(f\"- {label}: expected one of {', '.join(module_candidates)}\")\n",
    "        if last_error:\n",
    "            lines.append(f\"  last error: {last_error}\")\n",
    "\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"Re-run setup steps 1/7 -> 3/7 for core dependencies.\")\n",
    "    lines.append(\"If widgets are missing, also re-run setup steps 4/7 -> 6/7.\")\n",
    "    lines.append(\"If this repo is synced in Databricks, set NATURALIST_COMPANION_SRC to your repo src path if needed.\")\n",
    "    raise ModuleNotFoundError(\"\\n\".join(lines))\n",
    "\n",
    "print(\"[setup step 7/7] Dependency preflight passed.\")\n",
    "for label, module_name in resolved.items():\n",
    "    print(f\"  - {label}: {module_name}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#######################################################################################################\n",
    "\n",
    "###### Notebook Imports + Runtime Config                                                           ######\n",
    "\n",
    "#######################################################################################################\n",
    "\n",
    "\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from threading import Event, Thread\n",
    "from urllib.parse import quote, unquote, urlparse\n",
    "from urllib.request import Request, urlopen\n",
    "\n",
    "from IPython.display import Image, Markdown, display\n",
    "\n",
    "# Mitigate common macOS OpenMP duplicate-library crashes in notebook kernels.\n",
    "os.environ.setdefault(\"KMP_DUPLICATE_LIB_OK\", \"TRUE\")\n",
    "\n",
    "# Silence noisy tqdm widget warning in IDE notebooks when rich progress widgets are unavailable.\n",
    "warnings.filterwarnings(\"ignore\", message=\".*IProgress not found.*\")\n",
    "\n",
    "\n",
    "# LangChain moved WikipediaLoader in newer releases; keep backward compatibility.\n",
    "try:\n",
    "    from langchain_community.document_loaders import WikipediaLoader\n",
    "except ImportError:\n",
    "    from langchain.document_loaders import WikipediaLoader\n",
    "\n",
    "try:\n",
    "    from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "except ImportError:\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from databricks_langchain import ChatDatabricks, DatabricksEmbeddings\n",
    "\n",
    "\n",
    "def _start_heartbeat(label: str, every_s: float = 8.0):\n",
    "    stop = Event()\n",
    "\n",
    "    def _run():\n",
    "        elapsed = 0.0\n",
    "        while not stop.wait(every_s):\n",
    "            elapsed += every_s\n",
    "            print(f\"[{label}] still running... {elapsed:.0f}s elapsed\")\n",
    "\n",
    "    thread = Thread(target=_run, daemon=True)\n",
    "    thread.start()\n",
    "    return stop\n",
    "\n",
    "\n",
    "WIKIPEDIA_API_ENDPOINT = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "\n",
    "def _wiki_api_get(params):\n",
    "    query = \"&\".join(\n",
    "        f\"{quote(str(key), safe='')}={quote(str(value), safe='')}\" for key, value in params.items()\n",
    "    )\n",
    "    url = f\"{WIKIPEDIA_API_ENDPOINT}?{query}\"\n",
    "    req = Request(\n",
    "        url,\n",
    "        headers={\"User-Agent\": \"naturalist-companion/0.1 (notebook image preview)\"},\n",
    "    )\n",
    "    try:\n",
    "        with urlopen(req, timeout=10) as resp:\n",
    "            return json.loads(resp.read().decode(\"utf-8\"))\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "\n",
    "def _title_from_wikipedia_url(url):\n",
    "    parsed = urlparse(str(url or \"\"))\n",
    "    marker = \"/wiki/\"\n",
    "    if marker not in parsed.path:\n",
    "        return None\n",
    "    title = unquote(parsed.path.split(marker, 1)[1]).replace(\"_\", \" \").strip()\n",
    "    return title or None\n",
    "\n",
    "\n",
    "\n",
    "def _wiki_title_from_search(query):\n",
    "    payload = _wiki_api_get(\n",
    "        {\n",
    "            \"action\": \"query\",\n",
    "            \"list\": \"search\",\n",
    "            \"format\": \"json\",\n",
    "            \"formatversion\": 2,\n",
    "            \"srlimit\": 1,\n",
    "            \"srsearch\": query,\n",
    "        }\n",
    "    )\n",
    "    results = (payload.get(\"query\") or {}).get(\"search\") or []\n",
    "    if not results:\n",
    "        return None\n",
    "    title = str(results[0].get(\"title\") or \"\").strip()\n",
    "    return title or None\n",
    "\n",
    "\n",
    "def _iter_page_refs(items):\n",
    "    for item in items or []:\n",
    "        if isinstance(item, str):\n",
    "            raw = item.strip()\n",
    "            if not raw:\n",
    "                continue\n",
    "            title = _title_from_wikipedia_url(raw)\n",
    "            if not title:\n",
    "                title = _wiki_title_from_search(raw)\n",
    "            if title:\n",
    "                yield {\"title\": title, \"url\": f\"https://en.wikipedia.org/wiki/{title.replace(' ', '_')}\"}\n",
    "            continue\n",
    "\n",
    "        if isinstance(item, dict):\n",
    "            title = str(item.get(\"title\") or \"\").strip()\n",
    "            url = str(item.get(\"url\") or item.get(\"source\") or \"\").strip()\n",
    "            if not title and url:\n",
    "                title = _title_from_wikipedia_url(url) or \"\"\n",
    "            if title:\n",
    "                yield {\"title\": title, \"url\": url}\n",
    "            continue\n",
    "\n",
    "        metadata = getattr(item, \"metadata\", None) or {}\n",
    "        title = str(metadata.get(\"title\") or \"\").strip()\n",
    "        url = str(metadata.get(\"source\") or \"\").strip()\n",
    "        if not title and url:\n",
    "            title = _title_from_wikipedia_url(url) or \"\"\n",
    "        if title:\n",
    "            yield {\"title\": title, \"url\": url}\n",
    "\n",
    "\n",
    "def _wiki_thumbnail_for_title(title, thumb_px=640):\n",
    "    payload = _wiki_api_get(\n",
    "        {\n",
    "            \"action\": \"query\",\n",
    "            \"prop\": \"pageimages\",\n",
    "            \"format\": \"json\",\n",
    "            \"formatversion\": 2,\n",
    "            \"redirects\": 1,\n",
    "            \"piprop\": \"thumbnail|original\",\n",
    "            \"pithumbsize\": int(thumb_px),\n",
    "            \"titles\": title,\n",
    "        }\n",
    "    )\n",
    "    pages = (payload.get(\"query\") or {}).get(\"pages\") or []\n",
    "    for page in pages:\n",
    "        if not isinstance(page, dict):\n",
    "            continue\n",
    "        thumb = page.get(\"thumbnail\") or {}\n",
    "        original = page.get(\"original\") or {}\n",
    "        source = thumb.get(\"source\") or original.get(\"source\")\n",
    "        if source:\n",
    "            return str(source)\n",
    "    return None\n",
    "\n",
    "\n",
    "def display_wikipedia_images_for_pages(items, max_images=4, thumb_px=640):\n",
    "    seen = set()\n",
    "    shown = 0\n",
    "    for ref in _iter_page_refs(items):\n",
    "        title = ref[\"title\"]\n",
    "        if title in seen:\n",
    "            continue\n",
    "        seen.add(title)\n",
    "\n",
    "        image_url = _wiki_thumbnail_for_title(title, thumb_px=thumb_px)\n",
    "        if not image_url:\n",
    "            continue\n",
    "\n",
    "        shown += 1\n",
    "        page_url = ref.get(\"url\") or f\"https://en.wikipedia.org/wiki/{title.replace(' ', '_')}\"\n",
    "        display(Markdown(f\"**Wikipedia image preview: {title}**\"))\n",
    "        display(Image(url=image_url, width=min(int(thumb_px), 720)))\n",
    "        display(Markdown(f\"[Open page]({page_url})\"))\n",
    "\n",
    "        if shown >= int(max_images):\n",
    "            break\n",
    "\n",
    "    if shown == 0:\n",
    "        print(\"[wiki-images] No thumbnail images found for the selected pages.\")\n",
    "\n",
    "\n",
    "def _show_databricks_auth_status() -> None:\n",
    "    host = os.environ.get(\"DATABRICKS_HOST\", \"\").strip()\n",
    "    has_token = bool(os.environ.get(\"DATABRICKS_TOKEN\", \"\").strip())\n",
    "    in_runtime = bool(os.environ.get(\"DATABRICKS_RUNTIME_VERSION\", \"\").strip())\n",
    "\n",
    "    if in_runtime:\n",
    "        print(\"[env] Databricks runtime detected; workspace auth should be available via runtime context.\")\n",
    "        return\n",
    "\n",
    "    if host and has_token:\n",
    "        print(f\"[env] Databricks auth env detected: DATABRICKS_HOST={host}\")\n",
    "    else:\n",
    "        print(\"[env] Databricks auth env not fully set. For local runs set DATABRICKS_HOST and DATABRICKS_TOKEN.\")\n",
    "\n",
    "\n",
    "def _raise_databricks_hint(stage: str, endpoint: str, exc: Exception) -> None:\n",
    "    message = str(exc)\n",
    "    hints = [\n",
    "        f\"[{stage}] Databricks call failed for endpoint={endpoint!r}.\",\n",
    "        f\"Underlying error: {message}\",\n",
    "    ]\n",
    "\n",
    "    lower_msg = message.lower()\n",
    "    if \"invalid access token\" in lower_msg or \"403\" in lower_msg:\n",
    "        hints.append(\"Check that DATABRICKS_TOKEN is valid and matches DATABRICKS_HOST.\")\n",
    "        hints.append(\"In Free Edition, regenerate a PAT and retry if the old token is expired.\")\n",
    "    if \"resource_does_not_exist\" in lower_msg or \"404\" in lower_msg:\n",
    "        hints.append(\"Verify the endpoint name in Databricks Serving and update notebook env overrides.\")\n",
    "    if \"permission\" in lower_msg or \"can query\" in lower_msg:\n",
    "        hints.append(\"Ensure your user/token has Can Query permission on the endpoint.\")\n",
    "\n",
    "    raise RuntimeError(\"\\n\".join(hints)) from exc\n",
    "\n",
    "\n",
    "_show_databricks_auth_status()\n",
    "\n",
    "\n",
    "#######################################################################################################\n",
    "\n",
    "###### Config (Define LLMs, Embeddings, Vector Store, Data Loader specs)                          ######\n",
    "\n",
    "#######################################################################################################\n",
    "\n",
    "\n",
    "# DataLoader Config\n",
    "query_terms = [\n",
    "    \"roadcut\",\n",
    "    \"geology\",\n",
    "    \"sedimentary rock\",\n",
    "    \"stratigraphy\",\n",
    "]\n",
    "max_docs = 3  # Fast local iteration setting.\n",
    "\n",
    "# Stage 2 chunking + batching controls (keep small for interactive runs).\n",
    "chunk_size = 1200\n",
    "chunk_overlap = 150\n",
    "embedding_batch_size = 8\n",
    "\n",
    "\n",
    "# Retriever Config\n",
    "k = 1\n",
    "EMBEDDING_MODEL_ENDPOINT = os.environ.get(\"DATABRICKS_EMBEDDING_ENDPOINT\", \"databricks-bge-large-en\")\n",
    "\n",
    "\n",
    "# LLM Config\n",
    "LLM_ENDPOINT_NAME = os.environ.get(\"DATABRICKS_LLM_ENDPOINT\", \"databricks-meta-llama-3-1-8b-instruct\")\n",
    "TEMPERATURE = 0.0\n",
    "\n",
    "\n",
    "# Response style controls (Roadside Geology audience: curious drivers, practical field learners).\n",
    "RESPONSE_TONE = \"field-guide\"\n",
    "MAX_BULLETS_PER_SECTION = 4\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Prompt (Edit This Cell)\n",
    "\n",
    "Use the next code cell to set the active question(s).\n",
    "\n",
    "Question types this notebook is designed for:\n",
    "- Detour geology: legal pull-offs or short walks near a route segment\n",
    "- Safety-first prompts: where to stop and what to avoid roadside\n",
    "- Route constraints: city/exit anchors plus max detour minutes\n",
    "- Beginner field interpretation: what visual clues to look for and why they matter\n",
    "\n",
    "Tip: Include your nearest city or exit and your max detour time to improve stop recommendations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_question = \"I am on I-81 near Hagerstown with a 30-minute detour. Where can I safely stop to observe folded Valley-and-Ridge strata, and what exactly should I look for?\"\n",
    "\n",
    "example_questions = [\n",
    "    \"I am driving I-81 near Bristol, TN. Give me two legal pull-off stops where I can see clear sedimentary layering, and tell me exactly what to look for.\",\n",
    "    \"Near I-81 between Winchester and Strasburg, where can I safely stop to see Valley-and-Ridge structure, and what field clues confirm folding?\",\n",
    "    \"I have 45 minutes near Hagerstown, MD. What roadside geology stop gives the best payoff for a beginner, and what story does the outcrop tell?\",\n",
    "    \"Along I-81 in the Shenandoah Valley, point me to a short-walk stop to compare rock type and landform, then explain why that match matters.\",\n",
    "    \"On an I-81 drive day, suggest one stop where I can observe evidence of ancient seas or sediment transport, with specific visual clues.\",\n",
    "]\n",
    "\n",
    "# StateGraph run can use the same prompt by default; edit independently if desired.\n",
    "stategraph_question = example_question\n",
    "\n",
    "place_image_queries = [\n",
    "    \"Hagerstown, Maryland\",\n",
    "    \"Bristol, Tennessee\",\n",
    "    \"Winchester, Virginia\",\n",
    "    \"Strasburg, Virginia\",\n",
    "    \"Shenandoah Valley\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################\n",
    "\n",
    "###### Stage 1/3: Wikipedia Data Load                                                            ######\n",
    "\n",
    "#######################################################################################################\n",
    "\n",
    "\n",
    "from time import perf_counter\n",
    "\n",
    "print(\"[stage 1/3] Starting Wikipedia document load...\")\n",
    "query = \" \".join(query_terms) if isinstance(query_terms, list) else query_terms\n",
    "print(f\"[stage 1/3] query={query!r}, max_docs={max_docs}\")\n",
    "\n",
    "heartbeat = _start_heartbeat(\"stage 1/3 wikipedia load\", every_s=8.0)\n",
    "t0 = perf_counter()\n",
    "try:\n",
    "    docs = WikipediaLoader(query=query, load_max_docs=max_docs).load()\n",
    "finally:\n",
    "    heartbeat.set()\n",
    "t1 = perf_counter()\n",
    "\n",
    "print(f\"[stage 1/3] Loaded {len(docs)} document(s) in {t1 - t0:.2f}s\")\n",
    "if not docs:\n",
    "    raise RuntimeError(\"No documents loaded from Wikipedia. Adjust query_terms/max_docs and re-run stage 1.\")\n",
    "\n",
    "print(\"[stage 1/3] Sample titles:\")\n",
    "for i, doc in enumerate(docs[:3], start=1):\n",
    "    title = str((doc.metadata or {}).get(\"title\") or f\"doc_{i}\")\n",
    "    source = str((doc.metadata or {}).get(\"source\") or \"n/a\")\n",
    "    print(f\"  {i}. {title} ({source})\")\n",
    "\n",
    "print(\"[stage 1/3] Wikipedia image previews from loaded pages...\")\n",
    "display_wikipedia_images_for_pages(docs, max_images=min(3, len(docs)))\n",
    "\n",
    "print(\"[stage 1/3] Wikipedia image previews near place queries...\")\n",
    "display_wikipedia_images_for_pages(place_image_queries, max_images=min(5, len(place_image_queries)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################\n",
    "\n",
    "###### Stage 2/3: Build + Save FAISS Index, Then Retrieve                                        ######\n",
    "\n",
    "#######################################################################################################\n",
    "\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "\n",
    "if \"docs\" not in globals() or not docs:\n",
    "    raise RuntimeError(\"`docs` not found. Run Stage 1/3 first.\")\n",
    "\n",
    "print(f\"[stage 2/3] Building embeddings with endpoint={EMBEDDING_MODEL_ENDPOINT!r}...\")\n",
    "print(\n",
    "    f\"[stage 2/3] Chunking docs with chunk_size={chunk_size}, chunk_overlap={chunk_overlap}, \"\n",
    "    f\"embedding_batch_size={embedding_batch_size}\"\n",
    ")\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "split_docs = splitter.split_documents(docs)\n",
    "if not split_docs:\n",
    "    raise RuntimeError(\"Chunking produced 0 documents. Adjust chunk_size/chunk_overlap and retry.\")\n",
    "\n",
    "total_chars = sum(len(str(d.page_content or \"\")) for d in split_docs)\n",
    "print(f\"[stage 2/3] Prepared {len(split_docs)} chunk(s), total_chars={total_chars}\")\n",
    "\n",
    "embeddings = DatabricksEmbeddings(endpoint=EMBEDDING_MODEL_ENDPOINT)\n",
    "\n",
    "batch_size = max(1, int(embedding_batch_size))\n",
    "vector_store = None\n",
    "\n",
    "heartbeat = _start_heartbeat(\"stage 2/3 embedding/index\", every_s=8.0)\n",
    "t2 = perf_counter()\n",
    "try:\n",
    "    try:\n",
    "        for start in range(0, len(split_docs), batch_size):\n",
    "            batch = split_docs[start : start + batch_size]\n",
    "            b0 = perf_counter()\n",
    "            if vector_store is None:\n",
    "                vector_store = FAISS.from_documents(batch, embeddings)\n",
    "            else:\n",
    "                vector_store.add_documents(batch)\n",
    "            b1 = perf_counter()\n",
    "\n",
    "            done = min(start + batch_size, len(split_docs))\n",
    "            pct = (100.0 * done) / len(split_docs)\n",
    "            print(\n",
    "                f\"[stage 2/3] Embedded batch {start // batch_size + 1}: \"\n",
    "                f\"{done}/{len(split_docs)} chunks ({pct:.1f}%) in {b1 - b0:.2f}s\"\n",
    "            )\n",
    "    except Exception as exc:\n",
    "        _raise_databricks_hint(\"stage 2/3 embedding/index\", EMBEDDING_MODEL_ENDPOINT, exc)\n",
    "finally:\n",
    "    heartbeat.set()\n",
    "\n",
    "t3 = perf_counter()\n",
    "if vector_store is None:\n",
    "    raise RuntimeError(\"Vector store was not created.\")\n",
    "\n",
    "print(f\"[stage 2/3] Built FAISS index in {t3 - t2:.2f}s\")\n",
    "\n",
    "\n",
    "faiss_base = os.environ.get(\"ANC_FAISS_DIR\", \"\").strip()\n",
    "if faiss_base:\n",
    "    faiss_dir = (Path(faiss_base).expanduser() / \"anc_dbrx\").resolve()\n",
    "else:\n",
    "    faiss_dir = (Path.home() / \"DATA\" / \"naturalist-companion\" / \"faiss\" / \"anc_dbrx\").resolve()\n",
    "\n",
    "faiss_dir.mkdir(parents=True, exist_ok=True)\n",
    "vector_store.save_local(str(faiss_dir))\n",
    "print(f\"[stage 2/3] Saved FAISS index to: {faiss_dir}\")\n",
    "\n",
    "\n",
    "print(f\"[stage 2/3] Running similarity search for question={example_question!r}, k={k}...\")\n",
    "results = vector_store.similarity_search(example_question, k=k)\n",
    "print(f\"[stage 2/3] Retrieved {len(results)} result(s)\")\n",
    "\n",
    "for i, res in enumerate(results, start=1):\n",
    "    title = str((res.metadata or {}).get(\"title\") or f\"result_{i}\")\n",
    "    source = str((res.metadata or {}).get(\"source\") or \"n/a\")\n",
    "    snippet = str(res.page_content or \"\")[:220].replace(\"\\n\", \" \")\n",
    "    print(f\"  {i}. {title} ({source})\")\n",
    "    print(f\"     {snippet}...\")\n",
    "\n",
    "print(\"[stage 2/3] Wikipedia image previews from retrieved pages...\")\n",
    "display_wikipedia_images_for_pages(results, max_images=min(4, len(results)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################\n",
    "\n",
    "###### Stage 3/3: Generate Answer with ChatDatabricks                                            ######\n",
    "\n",
    "#######################################################################################################\n",
    "\n",
    "\n",
    "from time import perf_counter\n",
    "\n",
    "if \"vector_store\" not in globals():\n",
    "    raise RuntimeError(\"`vector_store` not found. Run Stage 2/3 first.\")\n",
    "\n",
    "print(f\"[stage 3/3] Generating answer with endpoint={LLM_ENDPOINT_NAME!r}...\")\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME, temperature=TEMPERATURE)\n",
    "\n",
    "voice_instructions = f\"\"\"\n",
    "You are writing in a concise Roadside Geology field-guide voice for curious drivers.\n",
    "Tone:\n",
    "- Plainspoken, observant, and practical (not academic).\n",
    "- Emphasize what can be seen from legal/safe pull-offs or short walks.\n",
    "- Explain key geology in everyday language, then add one precise term when useful.\n",
    "- Include safety and access realism (do not suggest unsafe roadside behavior).\n",
    "Output format:\n",
    "1) \"Where to stop\" (up to {MAX_BULLETS_PER_SECTION} bullets)\n",
    "2) \"What to look for\" (up to {MAX_BULLETS_PER_SECTION} bullets)\n",
    "3) \"Why it matters\" (2-4 sentences)\n",
    "4) \"Citations\" (Wikipedia URLs only)\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def _context_for_question(question: str, top_k: int = 2) -> str:\n",
    "    local_results = vector_store.similarity_search(question, k=max(1, top_k))\n",
    "    lines = []\n",
    "    for i, res in enumerate(local_results, start=1):\n",
    "        title = str((res.metadata or {}).get(\"title\") or f\"result_{i}\")\n",
    "        source = str((res.metadata or {}).get(\"source\") or \"n/a\")\n",
    "        snippet = str(res.page_content or \"\")[:450].replace(\"\\n\", \" \")\n",
    "        lines.append(f\"[{i}] {title} ({source}) :: {snippet}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def answer_question(question: str) -> str:\n",
    "    context_block = _context_for_question(question, top_k=max(1, k))\n",
    "    prompt = (\n",
    "        f\"Use only the provided Wikipedia-grounded context when you can.\\n\\n\"\n",
    "        f\"Question: {question}\\n\\n\"\n",
    "        f\"Context:\\n{context_block}\\n\\n\"\n",
    "        f\"Style requirements:\\n{voice_instructions}\"\n",
    "    )\n",
    "\n",
    "    heartbeat = _start_heartbeat(\"stage 3/3 llm\", every_s=8.0)\n",
    "    t0 = perf_counter()\n",
    "    try:\n",
    "        try:\n",
    "            response = llm.invoke(prompt)\n",
    "        except Exception as exc:\n",
    "            _raise_databricks_hint(\"stage 3/3 llm\", LLM_ENDPOINT_NAME, exc)\n",
    "    finally:\n",
    "        heartbeat.set()\n",
    "    dt = perf_counter() - t0\n",
    "    print(f\"[stage 3/3] LLM response received in {dt:.2f}s\")\n",
    "    return str(response.content)\n",
    "\n",
    "\n",
    "print(f\"[stage 3/3] Primary question:\\n- {example_question}\")\n",
    "primary_answer = answer_question(example_question)\n",
    "print(\"\\nAnswer:\\n\")\n",
    "print(primary_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################\n",
    "\n",
    "###### Stage 3b/3: Run All Example Questions                                                     ######\n",
    "\n",
    "#######################################################################################################\n",
    "\n",
    "\n",
    "if \"answer_question\" not in globals():\n",
    "    raise RuntimeError(\"`answer_question` not found. Run Stage 3/3 first.\")\n",
    "\n",
    "if \"example_questions\" not in globals() or not example_questions:\n",
    "    raise RuntimeError(\"`example_questions` is empty. Check config cell.\")\n",
    "\n",
    "all_answers = []\n",
    "print(f\"[stage 3b/3] Running {len(example_questions)} example question(s)...\")\n",
    "\n",
    "for i, q in enumerate(example_questions, start=1):\n",
    "    print(\"\\n\" + \"=\" * 110)\n",
    "    print(f\"[stage 3b/3] Question {i}/{len(example_questions)}\")\n",
    "    print(q)\n",
    "    print(\"=\" * 110)\n",
    "\n",
    "    answer = answer_question(q)\n",
    "    all_answers.append({\"question\": q, \"answer\": answer})\n",
    "\n",
    "    print(\"\\nResponse:\\n\")\n",
    "    print(answer)\n",
    "\n",
    "print(f\"\\n[stage 3b/3] Completed {len(all_answers)} question(s).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Canonical Workflow Diagram (StateGraph)\n",
    "\n",
    "StateGraph is the canonical workflow diagram for this notebook because it reflects the shared orchestration logic and is less likely to drift than a separate static diagram.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies should already be installed from the setup steps above.\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from IPython.display import Image, Markdown, display\n",
    "\n",
    "\n",
    "def _candidate_src_paths() -> list[Path]:\n",
    "    candidates: list[Path] = []\n",
    "\n",
    "    # Optional explicit override.\n",
    "    env_src = os.environ.get(\"NATURALIST_COMPANION_SRC\", \"\").strip()\n",
    "    if env_src:\n",
    "        candidates.append(Path(env_src))\n",
    "\n",
    "    # If running inside Databricks, derive repo path from notebook context.\n",
    "    try:\n",
    "        notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "        # Example notebook_path: /Repos/user/repo/notebooks/anc_dbrx\n",
    "        if \"/notebooks/\" in notebook_path:\n",
    "            repo_workspace_path = \"/Workspace\" + notebook_path.split(\"/notebooks/\", 1)[0]\n",
    "            candidates.append(Path(repo_workspace_path) / \"src\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Relative paths (works in local and some Databricks repo executions).\n",
    "    cwd = Path.cwd()\n",
    "    candidates.extend([\n",
    "        cwd / \"src\",\n",
    "        cwd.parent / \"src\",\n",
    "        cwd.parent.parent / \"src\",\n",
    "    ])\n",
    "\n",
    "    # Databricks Repos filesystem fallback scan.\n",
    "    repos_root = Path(\"/Workspace/Repos\")\n",
    "    if repos_root.exists():\n",
    "        for pkg_dir in repos_root.glob(\"*/*/src/naturalist_companion\"):\n",
    "            candidates.append(pkg_dir.parent)\n",
    "\n",
    "    # De-duplicate while preserving order.\n",
    "    deduped: list[Path] = []\n",
    "    seen: set[str] = set()\n",
    "    for item in candidates:\n",
    "        key = str(item)\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        deduped.append(item)\n",
    "    return deduped\n",
    "\n",
    "\n",
    "STATEGRAPH_AVAILABLE = False\n",
    "STATEGRAPH_SRC_PATH = None\n",
    "_stategraph_import_error = None\n",
    "\n",
    "for src_path in _candidate_src_paths():\n",
    "    if not (src_path / \"naturalist_companion\").exists():\n",
    "        continue\n",
    "    if str(src_path) not in sys.path:\n",
    "        sys.path.insert(0, str(src_path))\n",
    "    STATEGRAPH_SRC_PATH = src_path\n",
    "    break\n",
    "\n",
    "try:\n",
    "    from naturalist_companion.stategraph_shared import (\n",
    "        build_stategraph_app,\n",
    "        run_i81_eval_harness,\n",
    "        run_stategraph,\n",
    "    )\n",
    "    STATEGRAPH_AVAILABLE = True\n",
    "    if STATEGRAPH_SRC_PATH is not None:\n",
    "        print(f\"[stategraph] Loaded naturalist_companion from: {STATEGRAPH_SRC_PATH}\")\n",
    "    else:\n",
    "        print(\"[stategraph] Loaded naturalist_companion from current Python path\")\n",
    "except Exception as exc:\n",
    "    _stategraph_import_error = exc\n",
    "    STATEGRAPH_AVAILABLE = False\n",
    "    display(\n",
    "        Markdown(\n",
    "            \"**StateGraph module is unavailable in this workspace.**\\n\\n\"\n",
    "            \"Baseline Databricks stages (Wikipedia -> FAISS -> ChatDatabricks) still run.\\n\\n\"\n",
    "            \"If this repo is synced in Databricks Repos, open the notebook from the Repo path \"\n",
    "            \"(not a copied workspace file), or set `NATURALIST_COMPANION_SRC` to your repo `src` path.\"\n",
    "        )\n",
    "    )\n",
    "    print(f\"[stategraph] import error: {type(exc).__name__}: {exc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not STATEGRAPH_AVAILABLE:\n",
    "    print(\"[stategraph] Skipping canonical diagram: naturalist_companion not available.\")\n",
    "else:\n",
    "    provider = 'databricks'\n",
    "    app = build_stategraph_app(provider=provider)\n",
    "    print('Compiled StateGraph successfully for provider:', provider)\n",
    "\n",
    "    # Render a real image (PNG bytes) instead of plain Mermaid text.\n",
    "    try:\n",
    "        png_bytes = app.get_graph().draw_mermaid_png()\n",
    "        display(Image(data=png_bytes))\n",
    "    except Exception as exc:\n",
    "        display(Markdown(f'Graph render fallback (text). Error: `{type(exc).__name__}: {exc}`'))\n",
    "        print(app.get_graph().draw_mermaid())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not STATEGRAPH_AVAILABLE:\n",
    "    print(\"[stategraph] Skipping run_stategraph: naturalist_companion not available.\")\n",
    "else:\n",
    "    result = run_stategraph(\n",
    "        stategraph_question,\n",
    "        provider='databricks',\n",
    "        config={'artifact_root': 'out/stategraph/notebook_runs', 'max_retrieval_attempts': 3, 'citation_coverage_threshold': 0.80},\n",
    "    )\n",
    "    final_output = result['final_output']\n",
    "    print('Question:', stategraph_question)\n",
    "    print('Provider:', final_output['provider'])\n",
    "    print('Route:', final_output['route_decision']['decision'])\n",
    "    print('Quality passed:', final_output['quality']['passed'])\n",
    "    print('Attempts:', final_output['retrieval_attempts'])\n",
    "    print('Artifact dir:', result['artifact_dir'])\n",
    "    print('Response:')\n",
    "    print(final_output['answer']['response'])\n",
    "    print('Citation image previews:')\n",
    "    display_wikipedia_images_for_pages(final_output['answer'].get('citations', []), max_images=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not STATEGRAPH_AVAILABLE:\n",
    "    print(\"[stategraph] Skipping eval harness: naturalist_companion not available.\")\n",
    "else:\n",
    "    report = run_i81_eval_harness(\n",
    "        provider='databricks',\n",
    "        config={'artifact_root': 'out/stategraph/notebook_eval', 'max_retrieval_attempts': 3, 'citation_coverage_threshold': 0.80},\n",
    "    )\n",
    "    print(report['summary'])\n",
    "    print(report['artifact_root'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
