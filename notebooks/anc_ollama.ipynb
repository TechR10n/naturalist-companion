{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## A Geology Flaneur needs an Agentic Companion\n",
    "\n",
    "Imagine a traveler wearing **Apple Vision Pro**, moving through mountain corridors as a true **geology flaneur**: observing roadcuts, folds, and landforms in context, with grounded explanations layered into the moment.\n",
    "\n",
    "This notebook is the proof path toward that experience: route-aware field guidance, citation-backed answers, and quality-gated reasoning that can later power an immersive spatial interface."
   ],
   "id": "1127b2b6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<img src=\"../docs/banner-intro-image.png\" alt=\"Banner Intro\" style=\"display:block; margin: 0.35rem auto 0.75rem auto; width: 82%; max-width: 980px; height: auto;\"/>\n",
    "\n",
    "# ANC: Wikipedia with (FAISS, Ollama and StateGraph)\n",
    "\n",
    "This notebook combines two Ollama workflows in one place:\n",
    "\n",
    "- Baseline notebook pipeline: Wikipedia -> chunking -> FAISS -> local Q&A with `ChatOllama`\n",
    "- Canonical orchestration diagram + run flow via shared `naturalist_companion.stategraph_shared`\n"
   ],
   "id": "9d9050b2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POC Motivation and Evidence\n",
    "\n",
    "### Why this notebook exists\n",
    "This notebook is the local Ollama proof-of-concept for route-grounded geology guidance.\n",
    "It validates grounded retrieval + generation + quality checks before full app productionization.\n",
    "\n",
    "### Hypotheses under test\n",
    "1. Local Ollama embeddings and LLM models can produce useful grounded answers.\n",
    "2. Wikipedia-only retrieval can support safety-aware field guidance with citations.\n",
    "3. Shared orchestration (`stategraph_shared`) can enforce quality gates and stable output structure.\n",
    "\n",
    "### How to capture comparable evidence\n",
    "- Record Stage 1/2/3 and Stage 3b timings from notebook logs.\n",
    "- Run the StateGraph eval harness and capture pass rate + citation validity.\n",
    "- Keep model names/config values with each run artifact for apples-to-apples comparisons.\n",
    "\n",
    "### What this run does not prove yet\n",
    "- Multi-user serving scale and concurrency behavior\n",
    "- Persistent shared retrieval indexes for app traffic\n",
    "- End-to-end mobile app integration with route + camera UX\n",
    "- Production SLO/cost governance\n"
   ],
   "id": "479b3c52"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites (Run First)\n",
    "\n",
    "### Local Ollama setup\n",
    "\n",
    "- Ensure Ollama is installed and running: `ollama serve`\n",
    "- Pull the embedding model: `ollama pull nomic-embed-text`\n",
    "- Pull the chat model used in this notebook: `ollama pull llama3.1:8b`\n",
    "- Optional smaller/faster local model: `ollama pull llama3.2:3b`\n",
    "\n",
    "### Optional notebook installs\n",
    "\n",
    "Run the install cell below only if preflight reports missing packages.\n"
   ],
   "id": "ea98c294"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T23:40:38.575632Z",
     "start_time": "2026-02-06T23:40:37.141200Z"
    }
   },
   "source": [
    "# Uncomment in fresh environments or if preflight fails:\n",
    "\n",
    "%pip install -q -U -r ../requirements-ollama-dev.txt\n",
    "\n",
    "# Optional only if rich widgets are missing:\n",
    "# %pip install -q -U ipywidgets jupyterlab_widgets\n"
   ],
   "id": "f9560d65",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T23:40:38.589051Z",
     "start_time": "2026-02-06T23:40:38.583513Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import importlib\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Literal\n",
    "import warnings\n",
    "from threading import Event, Thread\n",
    "from IPython.display import Image, Markdown, display\n",
    "from time import perf_counter\n"
   ],
   "id": "1da175fc",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Dependency Preflight (Fail Fast)",
   "id": "00fa3f09"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T23:40:38.615604Z",
     "start_time": "2026-02-06T23:40:38.589500Z"
    }
   },
   "source": [
    "def _candidate_src_paths():\n",
    "    candidates = []\n",
    "\n",
    "    # Optional explicit override.\n",
    "    env_src = os.environ.get(\"NATURALIST_COMPANION_SRC\", \"\").strip()\n",
    "    if env_src:\n",
    "        candidates.append(Path(env_src))\n",
    "\n",
    "    cwd = Path.cwd()\n",
    "    candidates.extend([\n",
    "        cwd / \"src\",\n",
    "        cwd.parent / \"src\",\n",
    "        cwd.parent.parent / \"src\",\n",
    "    ])\n",
    "\n",
    "    # Optional scan for repository-style checkouts.\n",
    "    repos_root = Path(\"/Workspace/Repos\")\n",
    "    if repos_root.exists():\n",
    "        for pkg_dir in repos_root.glob(\"*/*/src/naturalist_companion\"):\n",
    "            candidates.append(pkg_dir.parent)\n",
    "\n",
    "    deduped = []\n",
    "    seen = set()\n",
    "    for item in candidates:\n",
    "        key = str(item)\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        deduped.append(item)\n",
    "    return deduped\n",
    "\n",
    "\n",
    "for src_path in _candidate_src_paths():\n",
    "    if (src_path / \"naturalist_companion\").exists() and str(src_path) not in sys.path:\n",
    "        sys.path.insert(0, str(src_path))\n",
    "        break\n",
    "\n",
    "\n",
    "CHECKS = [\n",
    "    (\"FAISS backend\", [\"faiss\"]),\n",
    "    (\"LangGraph runtime\", [\"langgraph\"]),\n",
    "    (\"Wikipedia loader module\", [\"langchain_community.document_loaders\", \"langchain.document_loaders\"]),\n",
    "    (\"Text splitter module\", [\"langchain_text_splitters\", \"langchain.text_splitter\"]),\n",
    "    (\"LangChain vectorstore module\", [\"langchain_community.vectorstores\"]),\n",
    "    (\"LangChain in-memory docstore module\", [\"langchain_community.docstore.in_memory\"]),\n",
    "    (\"Ollama integration\", [\"langchain_ollama\"]),\n",
    "    (\"Naturalist stategraph module\", [\"naturalist_companion.stategraph_shared\"]),\n",
    "]\n",
    "resolved = {}\n",
    "missing = []\n",
    "\n",
    "for label, module_candidates in CHECKS:\n",
    "    matched = None\n",
    "    last_error = None\n",
    "    for module_name in module_candidates:\n",
    "        try:\n",
    "            importlib.import_module(module_name)\n",
    "            matched = module_name\n",
    "            break\n",
    "        except ImportError as import_error:\n",
    "            last_error = f\"{type(import_error).__name__}: {import_error}\"\n",
    "\n",
    "    if matched is not None:\n",
    "        resolved[label] = matched\n",
    "    else:\n",
    "        missing.append((label, module_candidates, last_error))\n",
    "\n",
    "if missing:\n",
    "    non_stategraph_missing = [m for m in missing if m[0] != \"Naturalist stategraph module\"]\n",
    "    if non_stategraph_missing:\n",
    "        missing = non_stategraph_missing\n",
    "\n",
    "    lines = [\"[preflight] Missing required notebook dependencies:\"]\n",
    "    for label, module_candidates, last_error in missing:\n",
    "        lines.append(f\"- {label}: expected one of {', '.join(module_candidates)}\")\n",
    "        if last_error:\n",
    "            lines.append(f\"  last error: {last_error}\")\n",
    "\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"Run the install cell above, restart the kernel, and retry.\")\n",
    "    lines.append(\"Install hint: %pip install -q -U -r ../requirements-ollama-dev.txt\")\n",
    "    lines.append(\"Set NATURALIST_COMPANION_SRC to your repo src path if needed.\")\n",
    "    raise ModuleNotFoundError(\"\\n\".join(lines))\n",
    "\n",
    "print(\"[preflight] Dependency check passed.\")\n",
    "for label, module_name in resolved.items():\n",
    "    print(f\"  - {label}: {module_name}\")\n"
   ],
   "id": "82df00f3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[preflight] Dependency check passed.\n",
      "  - FAISS backend: faiss\n",
      "  - LangGraph runtime: langgraph\n",
      "  - Wikipedia loader module: langchain_community.document_loaders\n",
      "  - Text splitter module: langchain_text_splitters\n",
      "  - LangChain vectorstore module: langchain_community.vectorstores\n",
      "  - LangChain in-memory docstore module: langchain_community.docstore.in_memory\n",
      "  - Ollama integration: langchain_ollama\n",
      "  - Naturalist stategraph module: naturalist_companion.stategraph_shared\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Notebook Imports + Runtime Config",
   "id": "04442eb8"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T23:40:38.651752Z",
     "start_time": "2026-02-06T23:40:38.640696Z"
    }
   },
   "source": [
    "# Mitigate common macOS OpenMP duplicate-library crashes in notebook kernels.\n",
    "os.environ.setdefault(\"KMP_DUPLICATE_LIB_OK\", \"TRUE\")\n",
    "\n",
    "# Silence noisy tqdm widget warning in IDE notebooks when rich progress widgets are unavailable.\n",
    "warnings.filterwarnings(\"ignore\", message=\".*IProgress not found.*\")\n",
    "\n",
    "\n",
    "# LangChain moved WikipediaLoader in newer releases; keep backward compatibility.\n",
    "try:\n",
    "    from langchain_community.document_loaders import WikipediaLoader\n",
    "except ImportError:\n",
    "    from langchain.document_loaders import WikipediaLoader\n",
    "\n",
    "try:\n",
    "    from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "except ImportError:\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "\n",
    "\n",
    "def _start_heartbeat(task_label: str, every_s: float = 8.0):\n",
    "    stop = Event()\n",
    "\n",
    "    def _run():\n",
    "        elapsed = 0.0\n",
    "        while not stop.wait(every_s):\n",
    "            elapsed += every_s\n",
    "            print(f\"[{task_label}] still running... {elapsed:.0f}s elapsed\")\n",
    "\n",
    "    thread = Thread(target=_run, daemon=True)\n",
    "    thread.start()\n",
    "    return stop\n",
    "\n",
    "\n",
    "from naturalist_companion.wikipedia_tools import display_openstreetmap_for_pages, display_wikipedia_images_for_pages\n",
    "\n",
    "try:\n",
    "    from langchain_core.documents import Document\n",
    "except ImportError:\n",
    "    from langchain.schema import Document\n",
    "\n",
    "from naturalist_companion.fallback_data import fallback_wiki_pages\n",
    "\n",
    "\n",
    "STATEGRAPH_AVAILABLE = False\n",
    "_stategraph_import_error = None\n",
    "try:\n",
    "    from naturalist_companion.stategraph_shared import (\n",
    "        build_stategraph_app,\n",
    "        run_i81_eval_harness,\n",
    "        run_stategraph,\n",
    "    )\n",
    "    STATEGRAPH_AVAILABLE = True\n",
    "except Exception as stategraph_import_error:\n",
    "    _stategraph_import_error = stategraph_import_error\n",
    "    print(f\"[stategraph] import error: {type(stategraph_import_error).__name__}: {stategraph_import_error}\")\n",
    "    print(\"[stategraph] StateGraph cells can be skipped until this import works.\")\n",
    "\n",
    "\n",
    "def _show_ollama_status() -> None:\n",
    "    base_url = os.environ.get(\"OLLAMA_BASE_URL\", \"http://localhost:11434\").strip()\n",
    "    print(f\"[env] Ollama base URL: {base_url}\")\n",
    "    print(\"[env] Verify local models with: `ollama list`\")\n",
    "\n",
    "\n",
    "def _raise_ollama_hint(stage: str, model: str, base_url: str, error: Exception) -> None:\n",
    "    message = str(error)\n",
    "    hints = [\n",
    "        f\"[{stage}] Ollama call failed for model={model!r} at base_url={base_url!r}.\",\n",
    "        f\"Underlying error: {message}\",\n",
    "    ]\n",
    "\n",
    "    lower_msg = message.lower()\n",
    "    if \"connection\" in lower_msg or \"refused\" in lower_msg or \"timed out\" in lower_msg:\n",
    "        hints.append(\"Ensure Ollama is running and reachable: `ollama serve`.\")\n",
    "    if \"not found\" in lower_msg or \"pull\" in lower_msg:\n",
    "        hints.append(f\"Ensure the model exists locally: `ollama pull {model}`.\")\n",
    "\n",
    "    raise RuntimeError(\"\\n\".join(hints)) from error\n",
    "\n",
    "\n",
    "_show_ollama_status()\n"
   ],
   "id": "3f39cc43",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[env] Ollama base URL: http://localhost:11434\n",
      "[env] Verify local models with: `ollama list`\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Config (Define LLMs, Embeddings, Vector Store, Data Loader specs)",
   "id": "e37b8352"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T23:40:38.673991Z",
     "start_time": "2026-02-06T23:40:38.659574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# DataLoader Config\n",
    "query_terms = [\n",
    "    \"roadcut\",\n",
    "    \"geology\",\n",
    "    \"sedimentary rock\",\n",
    "    \"stratigraphy\",\n",
    "]\n",
    "max_docs = 12  # Realistic retrieval setting for richer context.\n",
    "\n",
    "# Stage 2 chunking + batching controls (keep small for interactive runs).\n",
    "chunk_size = 1200\n",
    "chunk_overlap = 150\n",
    "embedding_batch_size = 8\n",
    "\n",
    "\n",
    "# Retriever Config\n",
    "k = 4\n",
    "EMBEDDING_MODEL = os.environ.get(\"OLLAMA_EMBEDDING_MODEL\", \"nomic-embed-text\")\n",
    "OLLAMA_BASE_URL = os.environ.get(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n",
    "\n",
    "\n",
    "# LLM Config\n",
    "LLM_MODEL = os.environ.get(\"OLLAMA_LLM_MODEL\", \"llama3.1:8b\")\n",
    "TEMPERATURE = 0.25\n",
    "\n",
    "\n",
    "# Response style controls (Roadside Geology audience: curious drivers, practical field learners).\n",
    "RESPONSE_TONE = \"field-guide\"\n",
    "MAX_BULLETS_PER_SECTION = 4\n",
    "\n",
    "\n",
    "# Local artifact settings\n",
    "FAISS_NAMESPACE = \"anc_ollama\"\n",
    "\n",
    "\n",
    "# StateGraph Config\n",
    "STATEGRAPH_PROVIDER: Literal[\"ollama\"] = \"ollama\"\n",
    "STATEGRAPH_LIVE_MAX_DOCS = 16\n",
    "STATEGRAPH_COMMON_CONFIG = {\n",
    "    \"max_retrieval_attempts\": 3,\n",
    "    \"citation_coverage_threshold\": 0.80,\n",
    "    \"runtime_mode\": \"realistic\",\n",
    "    \"llm_temperature\": TEMPERATURE,\n",
    "    \"llm_model\": LLM_MODEL,\n",
    "    \"ollama_base_url\": OLLAMA_BASE_URL,\n",
    "    \"live_max_docs\": STATEGRAPH_LIVE_MAX_DOCS,\n",
    "}\n",
    "STATEGRAPH_RUN_CONFIG = {\"artifact_root\": \"out/stategraph/notebook_runs\", **STATEGRAPH_COMMON_CONFIG}\n",
    "STATEGRAPH_EVAL_CONFIG = {\"artifact_root\": \"out/stategraph/notebook_eval\", **STATEGRAPH_COMMON_CONFIG}\n"
   ],
   "id": "8406c49f",
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Prompt (Edit This Cell)\n",
    "\n",
    "Use the next code cell to set the active question(s).\n",
    "\n",
    "Question types this notebook is designed for:\n",
    "- Detour geology: legal pull-offs or short walks near a route segment\n",
    "- Safety-first prompts: where to stop and what to avoid roadside\n",
    "- Route constraints: city/exit anchors plus max detour minutes\n",
    "- Beginner field interpretation: what visual clues to look for and why they matter\n",
    "\n",
    "Tip: Include your nearest city or exit and your max detour time to improve stop recommendations.\n"
   ],
   "id": "2ebbec0b"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T23:40:41.973771Z",
     "start_time": "2026-02-06T23:40:38.674654Z"
    }
   },
   "source": [
    "example_question = \"I am on I-81 near Hagerstown with a 30-minute detour. Where can I safely stop to observe folded Valley-and-Ridge strata, and what exactly should I look for?\"\n",
    "\n",
    "example_questions = [\n",
    "    \"I am driving I-81 near Bristol, TN. Give me two legal pull-off stops where I can see clear sedimentary layering, and tell me exactly what to look for.\",\n",
    "    \"Near I-81 between Winchester and Strasbourg, where can I safely stop to see Valley-and-Ridge structure, and what field clues confirm folding?\",\n",
    "    \"I have 45 minutes near Hagerstown, MD. What roadside geology stop gives the best payoff for a beginner, and what story does the outcrop tell?\",\n",
    "    \"Along I-81 in the Shenandoah Valley, point me to a short-walk stop to compare rock type and landform, then explain why that match matters.\",\n",
    "    \"On an I-81 drive day, suggest one stop where I can observe evidence of ancient seas or sediment transport, with specific visual clues.\",\n",
    "]\n",
    "\n",
    "# StateGraph run can use the same prompt by default; edit independently if desired.\n",
    "stategraph_question = example_question\n",
    "\n",
    "def _parse_place_query_list(raw: str, max_items: int = 6) -> list[str]:\n",
    "    text = str(raw or \"\").strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    match = re.search(r\"\\[[\\s\\S]*\\]\", text)\n",
    "    if match:\n",
    "        text = match.group(0)\n",
    "    try:\n",
    "        data = json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        data = [ln.strip(\" -â€¢\\t\") for ln in text.splitlines() if ln.strip()]\n",
    "\n",
    "    out: list[str] = []\n",
    "    for item in data if isinstance(data, list) else []:\n",
    "        value = str(item or \"\").strip()\n",
    "        if not value:\n",
    "            continue\n",
    "        low = value.lower()\n",
    "        if any(token in low for token in (\"interstate\", \"highway\", \"i-81\", \"route \")):\n",
    "            continue\n",
    "        if value not in out:\n",
    "            out.append(value)\n",
    "        if len(out) >= int(max_items):\n",
    "            break\n",
    "    return out\n",
    "\n",
    "\n",
    "def _generate_place_image_queries_with_model(question: str, max_items: int = 6) -> list[str]:\n",
    "    prompt = (\n",
    "        \"Return only JSON: an array of concise Wikipedia search queries for NATURAL LANDSCAPES \"\n",
    "        \"near this route question. Exclude highways, interstates, and city-only queries. \"\n",
    "        \"Prefer valleys, ridges, mountains, parks, overlooks, and geologic landforms. \"\n",
    "        f\"Question: {question}\"\n",
    "    )\n",
    "    try:\n",
    "        planner = ChatOllama(model=LLM_MODEL, base_url=OLLAMA_BASE_URL, temperature=min(0.3, float(TEMPERATURE)))\n",
    "        response = planner.invoke(prompt)\n",
    "        raw = getattr(response, \"content\", response)\n",
    "        places = _parse_place_query_list(raw, max_items=max_items)\n",
    "        if places:\n",
    "            return places\n",
    "    except Exception as query_error:\n",
    "        print(f\"[query] place query generation fallback: {type(query_error).__name__}: {query_error}\")\n",
    "\n",
    "    return [\n",
    "        \"Shenandoah Valley overlooks\",\n",
    "        \"Blue Ridge Mountains viewpoints\",\n",
    "        \"Appalachian Valley and Ridge outcrops\",\n",
    "        \"Catoctin Mountain Park geology\",\n",
    "        \"Great North Mountain ridge overlook\",\n",
    "    ][: max(1, int(max_items))]\n",
    "\n",
    "\n",
    "place_image_queries = _generate_place_image_queries_with_model(example_question, max_items=6)\n",
    "print(\"[query] place_image_queries:\")\n",
    "for query_idx, query_text in enumerate(place_image_queries, start=1):\n",
    "    print(f\"  {query_idx}. {query_text}\")\n"
   ],
   "id": "57b68503",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[query] place_image_queries:\n",
      "  1. Here are some concise Wikipedia search queries for natural landscapes along your route:\n",
      "  2. 1. \"Valley-and-Ridge Province\"\n",
      "  3. 2. \"Folded Mountains\"\n",
      "  4. 3. \"Catoctin Ridge\"\n",
      "  5. 4. \"South Mountain (Maryland)\"\n",
      "  6. 5. \"Antietam National Battlefield\"\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Stage 1/3: Wikipedia Data Load",
   "id": "5d65925e"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2026-02-06T23:40:41.983627Z"
    }
   },
   "source": [
    "print(\"[stage 1/3] Starting document load (real Wikipedia first; fallback on problems)...\")\n",
    "query = \" \".join(query_terms) if isinstance(query_terms, list) else query_terms\n",
    "print(f\"[stage 1/3] query={query!r}, max_docs={max_docs}\")\n",
    "\n",
    "docs = []\n",
    "data_source = \"real_wikipedia\"\n",
    "fallback_reason = None\n",
    "\n",
    "stage1_heartbeat = _start_heartbeat(\"stage 1/3 wikipedia load\", every_s=8.0)\n",
    "t0 = perf_counter()\n",
    "try:\n",
    "    try:\n",
    "        docs = WikipediaLoader(query=query, load_max_docs=max_docs).load()\n",
    "    except Exception as load_error:\n",
    "        fallback_reason = f\"{type(load_error).__name__}: {load_error}\"\n",
    "        docs = []\n",
    "finally:\n",
    "    stage1_heartbeat.set()\n",
    "t1 = perf_counter()\n",
    "\n",
    "if docs:\n",
    "    data_source = \"real_wikipedia\"\n",
    "else:\n",
    "    data_source = \"fallback_data\"\n",
    "    if fallback_reason is None:\n",
    "        fallback_reason = \"WikipediaLoader returned 0 documents.\"\n",
    "\n",
    "    pages = fallback_wiki_pages()\n",
    "    docs = [\n",
    "        Document(\n",
    "            page_content=f\"{str(page.get('summary') or '').strip()}\\n\\n{str(page.get('content') or '').strip()}\".strip(),\n",
    "            metadata={\n",
    "                \"title\": str(page.get(\"title\") or f\"fallback_page_{pageid}\"),\n",
    "                \"source\": str(page.get(\"url\") or \"\"),\n",
    "                \"pageid\": int(page.get(\"pageid\") or pageid),\n",
    "                \"data_source\": \"fallback_data\",\n",
    "            },\n",
    "        )\n",
    "        for pageid, page in pages.items()\n",
    "    ]\n",
    "\n",
    "print(f\"[stage 1/3] Loaded {len(docs)} document(s) in {t1 - t0:.2f}s\")\n",
    "print(f\"[stage 1/3] data_source={data_source}\")\n",
    "if data_source == \"fallback_data\":\n",
    "    print(f\"[stage 1/3] FALLBACK ACTIVE: {fallback_reason}\")\n",
    "else:\n",
    "    print(\"[stage 1/3] Real Wikipedia data loaded successfully.\")\n",
    "\n",
    "if not docs:\n",
    "    raise RuntimeError(\"No documents available from real or fallback data. Check notebook environment and retry.\")\n",
    "\n",
    "print(\"[stage 1/3] Sample titles:\")\n",
    "for sample_idx, sample_doc in enumerate(docs[:3], start=1):\n",
    "    sample_title = str((sample_doc.metadata or {}).get(\"title\") or f\"doc_{sample_idx}\")\n",
    "    sample_source = str((sample_doc.metadata or {}).get(\"source\") or \"n/a\")\n",
    "    print(f\"  {sample_idx}. {sample_title} ({sample_source})\")\n",
    "\n",
    "print(\"[stage 1/3] Wikipedia image previews from loaded pages...\")\n",
    "display_wikipedia_images_for_pages(docs, max_images=min(3, len(docs)), prefer_landscape=True, route_hint=example_question)\n",
    "\n",
    "print(\"[stage 1/3] Wikipedia image previews near place queries...\")\n",
    "display_wikipedia_images_for_pages(place_image_queries, max_images=min(5, len(place_image_queries)), prefer_landscape=True, route_hint=example_question)\n",
    "print(\"[stage 1/3] OpenStreetMap previews for landscape points...\")\n",
    "display_openstreetmap_for_pages(place_image_queries or docs, max_points=10, prefer_landscape=True, route_hint=example_question)\n"
   ],
   "id": "1e87aa38",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stage 1/3] Starting document load (real Wikipedia first; fallback on problems)...\n",
      "[stage 1/3] query='roadcut geology sedimentary rock stratigraphy', max_docs=12\n",
      "[stage 1/3 wikipedia load] still running... 8s elapsed\n",
      "[stage 1/3] Loaded 12 document(s) in 8.41s\n",
      "[stage 1/3] data_source=real_wikipedia\n",
      "[stage 1/3] Real Wikipedia data loaded successfully.\n",
      "[stage 1/3] Sample titles:\n",
      "  1. Carboniferous (https://en.wikipedia.org/wiki/Carboniferous)\n",
      "  2. Paleomagnetism (https://en.wikipedia.org/wiki/Paleomagnetism)\n",
      "  3. Artinskian (https://en.wikipedia.org/wiki/Artinskian)\n",
      "[stage 1/3] Wikipedia image previews from loaded pages...\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Stage 2/3: Build + Save FAISS Index, Then Retrieve",
   "id": "b5b839f0"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if \"docs\" not in globals() or not docs:\n",
    "    raise RuntimeError(\"`docs` not found. Run Stage 1/3 first.\")\n",
    "\n",
    "print(f\"[stage 2/3] Building embeddings with model={EMBEDDING_MODEL!r} at base_url={OLLAMA_BASE_URL!r}...\")\n",
    "print(\n",
    "    f\"[stage 2/3] Chunking docs with chunk_size={chunk_size}, chunk_overlap={chunk_overlap}, \"\n",
    "    f\"embedding_batch_size={embedding_batch_size}\"\n",
    ")\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "split_docs = splitter.split_documents(docs)\n",
    "if not split_docs:\n",
    "    raise RuntimeError(\"Chunking produced 0 documents. Adjust chunk_size/chunk_overlap and retry.\")\n",
    "\n",
    "total_chars = sum(len(str(d.page_content or \"\")) for d in split_docs)\n",
    "print(f\"[stage 2/3] Prepared {len(split_docs)} chunk(s), total_chars={total_chars}\")\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL, base_url=OLLAMA_BASE_URL)\n",
    "\n",
    "batch_size = max(1, int(embedding_batch_size))\n",
    "vector_store = None\n",
    "\n",
    "stage2_heartbeat = _start_heartbeat(\"stage 2/3 embedding/index\", every_s=8.0)\n",
    "t2 = perf_counter()\n",
    "try:\n",
    "    try:\n",
    "        for start in range(0, len(split_docs), batch_size):\n",
    "            batch = split_docs[start : start + batch_size]\n",
    "            b0 = perf_counter()\n",
    "            if vector_store is None:\n",
    "                vector_store = FAISS.from_documents(batch, embeddings)\n",
    "            else:\n",
    "                vector_store.add_documents(batch)\n",
    "            b1 = perf_counter()\n",
    "\n",
    "            done = min(start + batch_size, len(split_docs))\n",
    "            pct = (100.0 * done) / len(split_docs)\n",
    "            print(\n",
    "                f\"[stage 2/3] Embedded batch {start // batch_size + 1}: \"\n",
    "                f\"{done}/{len(split_docs)} chunks ({pct:.1f}%) in {b1 - b0:.2f}s\"\n",
    "            )\n",
    "    except Exception as embedding_error:\n",
    "        _raise_ollama_hint(\"stage 2/3 embedding/index\", EMBEDDING_MODEL, OLLAMA_BASE_URL, embedding_error)\n",
    "finally:\n",
    "    stage2_heartbeat.set()\n",
    "\n",
    "t3 = perf_counter()\n",
    "if vector_store is None:\n",
    "    raise RuntimeError(\"Vector store was not created.\")\n",
    "\n",
    "print(f\"[stage 2/3] Built FAISS index in {t3 - t2:.2f}s\")\n",
    "\n",
    "\n",
    "faiss_base = os.environ.get(\"ANC_FAISS_DIR\", \"\").strip()\n",
    "if faiss_base:\n",
    "    faiss_dir = (Path(faiss_base).expanduser() / FAISS_NAMESPACE).resolve()\n",
    "else:\n",
    "    faiss_dir = (Path.home() / \"DATA\" / \"naturalist-companion\" / \"faiss\" / FAISS_NAMESPACE).resolve()\n",
    "\n",
    "faiss_dir.mkdir(parents=True, exist_ok=True)\n",
    "vector_store.save_local(str(faiss_dir))\n",
    "print(f\"[stage 2/3] Saved FAISS index to: {faiss_dir}\")\n",
    "\n",
    "\n",
    "print(f\"[stage 2/3] Running similarity search for question={example_question!r}, k={k}...\")\n",
    "results = vector_store.similarity_search(example_question, k=k)\n",
    "print(f\"[stage 2/3] Retrieved {len(results)} result(s)\")\n",
    "\n",
    "for result_idx, result_doc in enumerate(results, start=1):\n",
    "    result_title = str((result_doc.metadata or {}).get(\"title\") or f\"result_{result_idx}\")\n",
    "    result_source = str((result_doc.metadata or {}).get(\"source\") or \"n/a\")\n",
    "    result_snippet = str(result_doc.page_content or \"\")[:220].replace(\"\\n\", \" \")\n",
    "    print(f\"  {result_idx}. {result_title} ({result_source})\")\n",
    "    print(f\"     {result_snippet}...\")\n",
    "\n",
    "print(\"[stage 2/3] Wikipedia image previews from retrieved pages...\")\n",
    "display_wikipedia_images_for_pages(results, max_images=min(4, len(results)))\n"
   ],
   "id": "fd113809",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Stage 3/3: Generate Answer with ChatOllama\n"
   ],
   "id": "f4698fc5"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if \"vector_store\" not in globals():\n",
    "    raise RuntimeError(\"`vector_store` not found. Run Stage 2/3 first.\")\n",
    "\n",
    "print(f\"[stage 3/3] Generating answer with model={LLM_MODEL!r} at base_url={OLLAMA_BASE_URL!r}...\")\n",
    "llm = ChatOllama(model=LLM_MODEL, base_url=OLLAMA_BASE_URL, temperature=TEMPERATURE)\n",
    "\n",
    "voice_instructions = f\"\"\"\n",
    "You are writing in a concise Roadside Geology field-guide voice for curious drivers.\n",
    "Tone:\n",
    "- Plainspoken, observant, and practical (not academic).\n",
    "- Emphasize what can be seen from legal/safe pull-offs or short walks.\n",
    "- Explain key geology in everyday language, then add one precise term when useful.\n",
    "- Include safety and access realism (do not suggest unsafe roadside behavior).\n",
    "Output format:\n",
    "1) \"Where to stop\" (up to {MAX_BULLETS_PER_SECTION} bullets)\n",
    "2) \"What to look for\" (up to {MAX_BULLETS_PER_SECTION} bullets)\n",
    "3) \"Why it matters\" (2-4 sentences)\n",
    "4) \"Citations\" (Wikipedia URLs only)\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def _context_for_question(question: str, top_k: int = 2) -> str:\n",
    "    local_results = vector_store.similarity_search(question, k=max(1, top_k))\n",
    "    context_lines: list[str] = []\n",
    "    for context_idx, context_doc in enumerate(local_results, start=1):\n",
    "        context_title = str((context_doc.metadata or {}).get(\"title\") or f\"result_{context_idx}\")\n",
    "        context_source = str((context_doc.metadata or {}).get(\"source\") or \"n/a\")\n",
    "        context_snippet = str(context_doc.page_content or \"\")[:450].replace(\"\\n\", \" \")\n",
    "        context_lines.append(f\"[{context_idx}] {context_title} ({context_source}) :: {context_snippet}\")\n",
    "    return \"\\n\".join(context_lines)\n",
    "\n",
    "\n",
    "def answer_question(question: str) -> str:\n",
    "    context_block = _context_for_question(question, top_k=max(1, k))\n",
    "    prompt = (\n",
    "        f\"Use only the provided Wikipedia-grounded context when you can.\\n\\n\"\n",
    "        f\"Question: {question}\\n\\n\"\n",
    "        f\"Context:\\n{context_block}\\n\\n\"\n",
    "        f\"Style requirements:\\n{voice_instructions}\"\n",
    "    )\n",
    "\n",
    "    llm_heartbeat = _start_heartbeat(\"stage 3/3 llm\", every_s=8.0)\n",
    "    started_at = perf_counter()\n",
    "    llm_response = None\n",
    "    try:\n",
    "        try:\n",
    "            llm_response = llm.invoke(prompt)\n",
    "        except Exception as llm_error:\n",
    "            _raise_ollama_hint(\"stage 3/3 llm\", LLM_MODEL, OLLAMA_BASE_URL, llm_error)\n",
    "    finally:\n",
    "        llm_heartbeat.set()\n",
    "    dt = perf_counter() - started_at\n",
    "    print(f\"[stage 3/3] LLM response received in {dt:.2f}s\")\n",
    "    if llm_response is None:\n",
    "        raise RuntimeError(\"LLM did not return a response.\")\n",
    "    response_content = getattr(llm_response, \"content\", llm_response)\n",
    "    return str(response_content)\n",
    "\n",
    "\n",
    "print(f\"[stage 3/3] Primary question:\\n- {example_question}\")\n",
    "primary_answer = answer_question(example_question)\n",
    "print(\"\\nAnswer:\\n\")\n",
    "print(primary_answer)\n"
   ],
   "id": "cf2d609a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Stage 3b/3: Run All Example Questions",
   "id": "36802c47"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if \"answer_question\" not in globals():\n",
    "    raise RuntimeError(\"`answer_question` not found. Run Stage 3/3 first.\")\n",
    "\n",
    "if \"example_questions\" not in globals() or not example_questions:\n",
    "    raise RuntimeError(\"`example_questions` is empty. Check config cell.\")\n",
    "\n",
    "all_answers = []\n",
    "print(f\"[stage 3b/3] Running {len(example_questions)} example question(s)...\")\n",
    "\n",
    "for question_idx, question_text in enumerate(example_questions, start=1):\n",
    "    print(\"\\n\" + \"=\" * 110)\n",
    "    print(f\"[stage 3b/3] Question {question_idx}/{len(example_questions)}\")\n",
    "    print(question_text)\n",
    "    print(\"=\" * 110)\n",
    "\n",
    "    answer_text = answer_question(question_text)\n",
    "    all_answers.append({\"question\": question_text, \"answer\": answer_text})\n",
    "\n",
    "    print(\"\\nResponse:\\n\")\n",
    "    print(answer_text)\n",
    "\n",
    "print(f\"\\n[stage 3b/3] Completed {len(all_answers)} question(s).\")\n"
   ],
   "id": "4cdd5287",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Canonical Workflow Diagram (StateGraph)\n",
    "\n",
    "StateGraph remains the shared orchestration contract (routing/clarification, retries, quality gate, and final output schema).\n",
    "\n",
    "### Notebook-only workflow (local)\n",
    "- Refresh partitioned retrieval indexes in a notebook cell:\n",
    "  `refresh_retrieval_partitions(config={\"store_root\": \"out/stategraph_store\"}, runtime_mode=\"realistic\", partitions=[\"corridor_i81\"], force=False)`\n",
    "- Run eval harness in a notebook cell:\n",
    "  `run_i81_eval_harness(provider=\"ollama\", config={\"runtime_mode\": \"realistic\", \"retrieval_backend\": \"persistent\", \"store_root\": \"out/stategraph_store\", \"artifact_root\": \"out/stategraph_eval\"})`\n",
    "- Run release gate in a notebook cell before promotion:\n",
    "  `run_real_data_release_gate(provider=\"ollama\", config={\"runtime_mode\": \"realistic\", \"retrieval_backend\": \"persistent\", \"store_root\": \"out/stategraph_store\", \"artifact_root\": \"out/stategraph_release_gate\"})`\n",
    "- Use `runtime_mode=\"deterministic\"` when offline or when provider credentials are unavailable.\n"
   ],
   "id": "369498c2"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if STATEGRAPH_AVAILABLE:\n",
    "    print(\"[stategraph] Ready: naturalist_companion.stategraph_shared imported in setup cells.\")\n",
    "else:\n",
    "    display(\n",
    "        Markdown(\n",
    "            \"**StateGraph module is unavailable in this workspace.**\\n\\n\"\n",
    "            \"Baseline Ollama stages (Wikipedia -> FAISS -> ChatOllama) still run.\\n\\n\"\n",
    "            \"Set `NATURALIST_COMPANION_SRC` to your repo `src` path if needed, then rerun setup cells.\"\n",
    "        )\n",
    "    )\n",
    "    if _stategraph_import_error is not None:\n",
    "        print(f\"[stategraph] import error: {type(_stategraph_import_error).__name__}: {_stategraph_import_error}\")\n"
   ],
   "id": "c15a238d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if not STATEGRAPH_AVAILABLE:\n",
    "    print(\"[stategraph] Skipping canonical diagram: naturalist_companion not available.\")\n",
    "else:\n",
    "    provider: Literal[\"ollama\"] = STATEGRAPH_PROVIDER\n",
    "    app = build_stategraph_app(provider=provider)\n",
    "    print('Compiled StateGraph successfully for provider:', provider)\n",
    "\n",
    "    # Render a real image (PNG bytes) instead of plain Mermaid text.\n",
    "    try:\n",
    "        png_bytes = app.get_graph().draw_mermaid_png()\n",
    "        display(Image(data=png_bytes, width=880))\n",
    "    except Exception as graph_render_error:\n",
    "        display(Markdown(f'Graph render fallback (text). Error: `{type(graph_render_error).__name__}: {graph_render_error}`'))\n",
    "        print(app.get_graph().draw_mermaid())\n"
   ],
   "id": "4d3becdc",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if not STATEGRAPH_AVAILABLE:\n",
    "    print(\"[stategraph] Skipping run_stategraph: naturalist_companion not available.\")\n",
    "else:\n",
    "    result = run_stategraph(\n",
    "        stategraph_question,\n",
    "        provider=STATEGRAPH_PROVIDER,\n",
    "        config=STATEGRAPH_RUN_CONFIG,\n",
    "    )\n",
    "    final_output = result['final_output']\n",
    "    print('Question:', stategraph_question)\n",
    "    print('Provider:', final_output['provider'])\n",
    "    print('Route:', final_output['route_decision']['decision'])\n",
    "    print('Quality passed:', final_output['quality']['passed'])\n",
    "    quality_reasons = final_output['quality'].get('reasons', [])\n",
    "    print('Quality reasons:', ', '.join(quality_reasons) if quality_reasons else 'none')\n",
    "    print('Attempts:', final_output['retrieval_attempts'])\n",
    "    print('Artifact dir:', result['artifact_dir'])\n",
    "    print('Response:')\n",
    "    print(final_output['answer']['response'])\n",
    "    print('Citation image previews:')\n",
    "    display_wikipedia_images_for_pages(final_output['answer'].get('citations', []), max_images=4)\n"
   ],
   "id": "09aafabd",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if not STATEGRAPH_AVAILABLE:\n",
    "    print(\"[stategraph] Skipping eval harness: naturalist_companion not available.\")\n",
    "else:\n",
    "    report = run_i81_eval_harness(\n",
    "        provider=STATEGRAPH_PROVIDER,\n",
    "        config=STATEGRAPH_EVAL_CONFIG,\n",
    "    )\n",
    "    print(report['summary'])\n",
    "    print(report['artifact_root'])\n"
   ],
   "id": "61fe56d2",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
