{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fbb81d5",
   "metadata": {},
   "source": [
    "# ANC Ollama: Wikipedia + FAISS + Ollama (Databricks-Parity)\n",
    "\n",
    "This notebook mirrors the `Config` and `Wikipedia Data Loader` sections from\n",
    "`docs/ideation-kernel/01_agentic_wikipedia_aimpoint_interview.ipynb`, but swaps\n",
    "Databricks components for local Ollama components on macOS.\n",
    "\n",
    "Provider mapping used here:\n",
    "- `ChatDatabricks` -> `ChatOllama`\n",
    "- `DatabricksEmbeddings` -> `OllamaEmbeddings`\n",
    "- Same `WikipediaLoader` + `FAISS` retrieval flow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c3a58d",
   "metadata": {},
   "source": [
    "## PlantUML Workflow Diagram\n",
    "\n",
    "```plantuml\n",
    "@startuml\n",
    "title ANC Ollama Databricks-Parity Flow\n",
    "start\n",
    ":Define config;\n",
    ":Load Wikipedia docs;\n",
    ":Embed docs with OllamaEmbeddings;\n",
    ":Index docs in FAISS;\n",
    ":Run similarity search;\n",
    ":Answer with ChatOllama;\n",
    "stop\n",
    "@enduml\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "48420580",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T21:05:38.110032Z",
     "iopub.status.busy": "2026-02-05T21:05:38.109927Z",
     "iopub.status.idle": "2026-02-05T21:05:38.112814Z",
     "shell.execute_reply": "2026-02-05T21:05:38.112326Z"
    },
    "ExecuteTime": {
     "end_time": "2026-02-05T21:43:28.618105Z",
     "start_time": "2026-02-05T21:43:28.603524Z"
    }
   },
   "source": [
    "# Uncomment in fresh environments:\n",
    "# %pip install -q -r ../requirements-ollama-dev.txt\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "fe3a2aaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T21:05:38.114380Z",
     "iopub.status.busy": "2026-02-05T21:05:38.114296Z",
     "iopub.status.idle": "2026-02-05T21:05:38.329496Z",
     "shell.execute_reply": "2026-02-05T21:05:38.329175Z"
    },
    "ExecuteTime": {
     "end_time": "2026-02-05T21:43:28.690592Z",
     "start_time": "2026-02-05T21:43:28.619654Z"
    }
   },
   "source": [
    "#######################################################################################################\n",
    "###### Python Package Imports for this notebook                                                  ######\n",
    "#######################################################################################################\n",
    "\n",
    "# LangChain moved WikipediaLoader in newer releases; keep backward compatibility.\n",
    "try:\n",
    "    from langchain_community.document_loaders import WikipediaLoader\n",
    "except ImportError:\n",
    "    from langchain.document_loaders import WikipediaLoader\n",
    "\n",
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "\n",
    "\n",
    "#######################################################################################################\n",
    "###### Config (Define LLMs, Embeddings, Vector Store, Data Loader specs)                         ######\n",
    "#######################################################################################################\n",
    "\n",
    "# DataLoader Config\n",
    "query_terms = [\n",
    "    \"roadcut\",\n",
    "    \"geology\",\n",
    "    \"sedimentary rock\",\n",
    "    \"stratigraphy\",\n",
    "]\n",
    "max_docs = 10  # Start small while iterating locally.\n",
    "\n",
    "# Retriever Config\n",
    "k = 2\n",
    "EMBEDDING_MODEL = \"nomic-embed-text\"\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "\n",
    "# LLM Config\n",
    "LLM_MODEL = \"llama3.1:8b\"\n",
    "TEMPERATURE = 0.0\n",
    "\n",
    "example_question = \"What is a roadcut in geology?\"\n"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocument_loaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WikipediaLoader\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain_community'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocument_loaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WikipediaLoader\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocument_loaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WikipediaLoader\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfaiss\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocstore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01min_memory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InMemoryDocstore\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain'"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "81c44851",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T21:05:38.330669Z",
     "iopub.status.busy": "2026-02-05T21:05:38.330610Z",
     "iopub.status.idle": "2026-02-05T21:05:38.334419Z",
     "shell.execute_reply": "2026-02-05T21:05:38.333979Z"
    }
   },
   "source": [
    "#######################################################################################################\n",
    "###### Wikipedia Data Loader                                                                     ######\n",
    "#######################################################################################################\n",
    "\n",
    "query = \" \".join(query_terms) if isinstance(query_terms, list) else query_terms\n",
    "docs = WikipediaLoader(query=query, load_max_docs=max_docs).load()\n",
    "print(f\"Loaded {len(docs)} Wikipedia documents for query: {query!r}\")\n",
    "\n",
    "#######################################################################################################\n",
    "###### FAISS Retriever: Using Ollama embedding model                                             ######\n",
    "#######################################################################################################\n",
    "\n",
    "# Define embeddings and the FAISS vector store.\n",
    "embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL, base_url=OLLAMA_BASE_URL)\n",
    "vector_store = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# Example of how to invoke the vector store.\n",
    "results = vector_store.similarity_search(example_question, k=k)\n",
    "for i, res in enumerate(results, start=1):\n",
    "    title = res.metadata.get(\"title\", \"unknown\")\n",
    "    source = res.metadata.get(\"source\", \"n/a\")\n",
    "    snippet = res.page_content[:220].replace(\"\\n\", \" \")\n",
    "    print(f\"{i}. {title} ({source})\")\n",
    "    print(f\"   {snippet}...\")\n",
    "\n",
    "#######################################################################################################\n",
    "###### LLM: Using local Ollama model                                                             ######\n",
    "#######################################################################################################\n",
    "\n",
    "llm = ChatOllama(model=LLM_MODEL, base_url=OLLAMA_BASE_URL, temperature=TEMPERATURE)\n",
    "response = llm.invoke(\n",
    "    f\"Answer concisely using Wikipedia-grounded context. Question: {example_question}\"\n",
    ")\n",
    "print(\"\\n\", response.content)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "16f56514",
   "metadata": {},
   "source": [
    "## Local Ollama Prerequisites (macOS)\n",
    "\n",
    "- Ensure Ollama is installed and running: `ollama serve`\n",
    "- Pull the embedding model: `ollama pull nomic-embed-text`\n",
    "- Pull the chat model: `ollama pull llama3.1:8b`\n",
    "- Keep `notebooks/anc_dbrx.ipynb` for Databricks-specific libraries and endpoints\n",
    "- Keep `notebooks/anc_gcp.ipynb` for Vertex AI-specific libraries and endpoints\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
