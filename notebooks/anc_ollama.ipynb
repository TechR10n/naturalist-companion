{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0fbb81d5",
      "metadata": {},
      "source": [
        "# ANC Ollama: Wikipedia + FAISS + Ollama (Databricks-Parity)\n",
        "\n",
        "This notebook mirrors the `Config` and `Wikipedia Data Loader` sections from\n",
        "`docs/ideation-kernel/01_agentic_wikipedia_aimpoint_interview.ipynb`, but swaps\n",
        "Databricks components for local Ollama components on macOS.\n",
        "\n",
        "Provider mapping used here:\n",
        "- `ChatDatabricks` -> `ChatOllama`\n",
        "- `DatabricksEmbeddings` -> `OllamaEmbeddings`\n",
        "- Same `WikipediaLoader` + `FAISS` retrieval flow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6c3a58d",
      "metadata": {},
      "source": [
        "## Workflow Diagram\n",
        "\n",
        "This diagram matches the current notebook flow:\n",
        "\n",
        "1. Define config (`query_terms`, `max_docs`, `k`, embedding model, chat model).\n",
        "2. Load Wikipedia documents with `WikipediaLoader`.\n",
        "3. Embed documents with `OllamaEmbeddings`.\n",
        "4. Build FAISS index and persist it to disk (`~/DATA/naturalist-companion/faiss/anc_ollama` by default).\n",
        "5. Run similarity search for the example question.\n",
        "6. Generate an answer with `ChatOllama`.\n",
        "\n",
        "Run the next code cell to render the diagram inline.\n",
        "\n",
        "Execution is split into `Stage 1/3`, `Stage 2/3`, and `Stage 3/3` with timing/status prints.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from IPython.display import Markdown, SVG, display\n",
        "import requests\n",
        "\n",
        "PLANTUML_SOURCE = \"\"\"\n",
        "@startuml\n",
        "title ANC Ollama Notebook Flow\n",
        "start\n",
        ":Define config;\n",
        ":Load Wikipedia docs;\n",
        ":Embed docs with OllamaEmbeddings;\n",
        ":Build FAISS index;\n",
        ":Persist FAISS index to ~/DATA or ANC_FAISS_DIR;\n",
        ":Run similarity search;\n",
        ":Answer with ChatOllama;\n",
        "stop\n",
        "@enduml\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def render_plantuml(source: str) -> None:\n",
        "    errors = []\n",
        "\n",
        "    # Preferred path: Python PlantUML client (if installed).\n",
        "    try:\n",
        "        from plantuml import PlantUML  # type: ignore\n",
        "\n",
        "        client = PlantUML(url=\"http://www.plantuml.com/plantuml/svg/\")\n",
        "        svg_url = client.get_url(source)\n",
        "        resp = requests.get(svg_url, timeout=20)\n",
        "        resp.raise_for_status()\n",
        "        display(SVG(resp.text))\n",
        "        return\n",
        "    except Exception as exc:\n",
        "        errors.append(f\"plantuml library renderer failed: {exc}\")\n",
        "\n",
        "    # Fallback path: Kroki PlantUML HTTP endpoint.\n",
        "    try:\n",
        "        resp = requests.post(\n",
        "            \"https://kroki.io/plantuml/svg\",\n",
        "            data=source.encode(\"utf-8\"),\n",
        "            headers={\"Content-Type\": \"text/plain\"},\n",
        "            timeout=20,\n",
        "        )\n",
        "        resp.raise_for_status()\n",
        "        display(SVG(resp.text))\n",
        "        return\n",
        "    except Exception as exc:\n",
        "        errors.append(f\"kroki renderer failed: {exc}\")\n",
        "\n",
        "    display(Markdown(\"PlantUML render failed. Showing source and errors.\"))\n",
        "    for item in errors:\n",
        "        print(item)\n",
        "    print('\\nPlantUML source:\\n')\n",
        "    print(source)\n",
        "\n",
        "\n",
        "render_plantuml(PLANTUML_SOURCE)\n"
      ],
      "id": "6eeb1bdfd0767668",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "48420580",
      "metadata": {},
      "source": [
        "# Uncomment in fresh environments:\n",
        "# %pip install -q -r ../requirements-ollama-dev.txt\n",
        "# %pip install -q plantuml\n",
        "# %pip install -q ipywidgets\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "fe3a2aaf",
      "metadata": {},
      "source": [
        "#######################################################################################################\n",
        "###### Python Package Imports for this notebook                                                  ######\n",
        "#######################################################################################################\n",
        "\n",
        "# LangChain moved WikipediaLoader in newer releases; keep backward compatibility.\n",
        "try:\n",
        "    from langchain_community.document_loaders import WikipediaLoader\n",
        "except ImportError:\n",
        "    from langchain.document_loaders import WikipediaLoader\n",
        "\n",
        "import faiss\n",
        "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
        "\n",
        "\n",
        "#######################################################################################################\n",
        "###### Config (Define LLMs, Embeddings, Vector Store, Data Loader specs)                         ######\n",
        "#######################################################################################################\n",
        "\n",
        "# DataLoader Config\n",
        "query_terms = [\n",
        "    \"roadcut\",\n",
        "    \"geology\",\n",
        "    \"sedimentary rock\",\n",
        "    \"stratigraphy\",\n",
        "]\n",
        "max_docs = 3  # Fast local iteration setting.\n",
        "\n",
        "# Retriever Config\n",
        "k = 1\n",
        "EMBEDDING_MODEL = \"nomic-embed-text\"\n",
        "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
        "\n",
        "# LLM Config\n",
        "LLM_MODEL = \"llama3.2:3b\"\n",
        "TEMPERATURE = 0.0\n",
        "\n",
        "example_question = \"What is a roadcut in geology?\"\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#######################################################################################################\n",
        "###### Stage 1/3: Wikipedia Data Load                                                            ######\n",
        "#######################################################################################################\n",
        "\n",
        "from time import perf_counter\n",
        "\n",
        "print(\"[stage 1/3] Starting Wikipedia document load...\")\n",
        "query = \" \".join(query_terms) if isinstance(query_terms, list) else query_terms\n",
        "print(f\"[stage 1/3] query={query!r}, max_docs={max_docs}\")\n",
        "\n",
        "t0 = perf_counter()\n",
        "docs = WikipediaLoader(query=query, load_max_docs=max_docs).load()\n",
        "t1 = perf_counter()\n",
        "\n",
        "print(f\"[stage 1/3] Loaded {len(docs)} document(s) in {t1 - t0:.2f}s\")\n",
        "if not docs:\n",
        "    raise RuntimeError(\"No documents loaded from Wikipedia. Adjust query_terms/max_docs and re-run stage 1.\")\n",
        "\n",
        "print(\"[stage 1/3] Sample titles:\")\n",
        "for i, doc in enumerate(docs[:3], start=1):\n",
        "    title = str((doc.metadata or {}).get(\"title\") or f\"doc_{i}\")\n",
        "    source = str((doc.metadata or {}).get(\"source\") or \"n/a\")\n",
        "    print(f\"  {i}. {title} ({source})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#######################################################################################################\n",
        "###### Stage 2/3: Build + Save FAISS Index, Then Retrieve                                        ######\n",
        "#######################################################################################################\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "from time import perf_counter\n",
        "\n",
        "if \"docs\" not in globals() or not docs:\n",
        "    raise RuntimeError(\"`docs` not found. Run Stage 1/3 first.\")\n",
        "\n",
        "print(f\"[stage 2/3] Building embeddings with model={EMBEDDING_MODEL!r} at {OLLAMA_BASE_URL!r}...\")\n",
        "embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL, base_url=OLLAMA_BASE_URL)\n",
        "\n",
        "t2 = perf_counter()\n",
        "vector_store = FAISS.from_documents(docs, embeddings)\n",
        "t3 = perf_counter()\n",
        "print(f\"[stage 2/3] Built FAISS index in {t3 - t2:.2f}s\")\n",
        "\n",
        "faiss_base = os.environ.get(\"ANC_FAISS_DIR\", \"\").strip()\n",
        "if faiss_base:\n",
        "    faiss_dir = (Path(faiss_base).expanduser() / \"anc_ollama\").resolve()\n",
        "else:\n",
        "    faiss_dir = (Path.home() / \"DATA\" / \"naturalist-companion\" / \"faiss\" / \"anc_ollama\").resolve()\n",
        "\n",
        "faiss_dir.mkdir(parents=True, exist_ok=True)\n",
        "vector_store.save_local(str(faiss_dir))\n",
        "print(f\"[stage 2/3] Saved FAISS index to: {faiss_dir}\")\n",
        "\n",
        "print(f\"[stage 2/3] Running similarity search for question={example_question!r}, k={k}...\")\n",
        "results = vector_store.similarity_search(example_question, k=k)\n",
        "print(f\"[stage 2/3] Retrieved {len(results)} result(s)\")\n",
        "\n",
        "for i, res in enumerate(results, start=1):\n",
        "    title = str((res.metadata or {}).get(\"title\") or f\"result_{i}\")\n",
        "    source = str((res.metadata or {}).get(\"source\") or \"n/a\")\n",
        "    snippet = str(res.page_content or \"\")[:220].replace(\"\\n\", \" \")\n",
        "    print(f\"  {i}. {title} ({source})\")\n",
        "    print(f\"     {snippet}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#######################################################################################################\n",
        "###### Stage 3/3: Generate Answer with ChatOllama                                                ######\n",
        "#######################################################################################################\n",
        "\n",
        "from time import perf_counter\n",
        "\n",
        "if \"results\" not in globals():\n",
        "    raise RuntimeError(\"`results` not found. Run Stage 2/3 first.\")\n",
        "\n",
        "print(f\"[stage 3/3] Generating answer with model={LLM_MODEL!r} at {OLLAMA_BASE_URL!r}...\")\n",
        "llm = ChatOllama(model=LLM_MODEL, base_url=OLLAMA_BASE_URL, temperature=TEMPERATURE)\n",
        "\n",
        "prompt = f\"Answer concisely using Wikipedia-grounded context. Question: {example_question}\"\n",
        "t4 = perf_counter()\n",
        "response = llm.invoke(prompt)\n",
        "t5 = perf_counter()\n",
        "\n",
        "print(f\"[stage 3/3] LLM response received in {t5 - t4:.2f}s\")\n",
        "print(\"\\nAnswer:\\n\")\n",
        "print(response.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16f56514",
      "metadata": {},
      "source": [
        "## Local Ollama Prerequisites (macOS)\n",
        "\n",
        "- Ensure Ollama is installed and running: `ollama serve`\n",
        "- Pull the embedding model: `ollama pull nomic-embed-text`\n",
        "- Pull the chat model used in this notebook: `ollama pull llama3.2:3b`\n",
        "- Keep `notebooks/anc_dbrx.ipynb` for Databricks-specific libraries and endpoints\n",
        "- Keep `notebooks/anc_gcp.ipynb` for Vertex AI-specific libraries and endpoints\n",
        "\n",
        "- If you hit macOS OpenMP kernel crashes, launch Jupyter with `KMP_DUPLICATE_LIB_OK=TRUE`.\n",
        "- Optional PlantUML Python library: `pip install plantuml`\n",
        "\n",
        "- Optional higher-quality model for final checks: `ollama pull llama3.1:8b`\n",
        "- If you see `TqdmWarning: IProgress not found`, install widgets in this env: `pip install ipywidgets`, then restart kernel.\n",
        "- The tqdm warning is non-fatal; notebook execution still works without widget progress bars.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
